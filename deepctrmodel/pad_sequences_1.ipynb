{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa67662-0019-4f4c-ae07-4a5c444c9a8b",
   "metadata": {},
   "source": [
    "### 1. 变长序列的处理方式理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124c136-7560-4a0d-bade-d3e65e5a1f13",
   "metadata": {},
   "source": [
    "变长序列的处理逻辑就是：\n",
    "\n",
    "👉 虽然原始的变长序列（如 genres、keywords）在输入时是一个序列，但在经过 embedding 和 pooling（这里是 mean pooling）之后，会被压缩成一个定长向量，相当于一个“整体语义”表示。\n",
    "换句话说：\n",
    "\n",
    "原始序列（如 ['动作', '冒险', '科幻']）\n",
    "⬇️\n",
    "通过 Embedding → 变成 (batch_size, seq_len, embedding_dim) 的张量\n",
    "⬇️\n",
    "通过 mean pooling → 汇总成 (batch_size, 1, embedding_dim)\n",
    "⬇️\n",
    "和其它稀疏特征一起参与拼接、建模\n",
    "所以最终：\n",
    "\n",
    "无论是稀疏特征（单个索引）还是变长特征（序列），最终都统一成 (batch_size, 1, embedding_dim) 的形式，这样才能拼接起来喂给 FM 和 DNN。\n",
    "\n",
    "这种做法其实就是“把变长特征转化为定长特征”的通用套路之一（还有比如 attention pooling、CNN、RNN 都可以做类似事情）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34f13-7086-4c45-91bd-2e114dc9aa10",
   "metadata": {},
   "source": [
    "### 2. 整体程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d75a1f-a740-4b19-ab6e-e8a3314bb925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14.1148 - mse: 14.1148 - val_loss: 13.2294 - val_mse: 13.2294\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.0121 - mse: 14.0121 - val_loss: 13.1344 - val_mse: 13.1344\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.9067 - mse: 13.9067 - val_loss: 13.0361 - val_mse: 13.0361\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.7975 - mse: 13.7975 - val_loss: 12.9349 - val_mse: 12.9349\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6829 - mse: 13.6829 - val_loss: 12.8296 - val_mse: 12.8296\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.5639 - mse: 13.5639 - val_loss: 12.7197 - val_mse: 12.7197\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.4389 - mse: 13.4389 - val_loss: 12.6029 - val_mse: 12.6029\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.3079 - mse: 13.3079 - val_loss: 12.4807 - val_mse: 12.4807\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.1705 - mse: 13.1705 - val_loss: 12.3518 - val_mse: 12.3518\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.0259 - mse: 13.0259 - val_loss: 12.2161 - val_mse: 12.2161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d807caaca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]\n",
    "\n",
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}\n",
    "\n",
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）\n",
    "\n",
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num\n",
    "\n",
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "\n",
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) 从这可以看出边长序列的字段数据最终整体也是当成一个字段处理\n",
    "    embeds.append(pooled_embed)\n",
    "\n",
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "\n",
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)\n",
    "\n",
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# 7. 编译并训练模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# 构建模型输入字典\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_len)\n",
    "\n",
    "# 模型训练\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056212f-1c43-4547-8fa5-631d52962be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d93b4a-53c5-4bec-86ac-ca34f4f48d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc5e8d33-179a-4e53-a6fb-3c04d6105d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 14.3441 - mse: 14.3441 - val_loss: 13.4779 - val_mse: 13.4779\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.2641 - mse: 14.2641 - val_loss: 13.4092 - val_mse: 13.4092\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.1868 - mse: 14.1868 - val_loss: 13.3433 - val_mse: 13.3433\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 14.1119 - mse: 14.1119 - val_loss: 13.2792 - val_mse: 13.2792\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.0390 - mse: 14.0390 - val_loss: 13.2122 - val_mse: 13.2122\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.9657 - mse: 13.9657 - val_loss: 13.1433 - val_mse: 13.1433\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 13.8919 - mse: 13.8919 - val_loss: 13.0734 - val_mse: 13.0734\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.8160 - mse: 13.8160 - val_loss: 13.0024 - val_mse: 13.0024\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.7372 - mse: 13.7372 - val_loss: 12.9296 - val_mse: 12.9296\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6554 - mse: 13.6554 - val_loss: 12.8539 - val_mse: 12.8539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2870a685910>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]\n",
    "\n",
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}\n",
    "\n",
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）\n",
    "\n",
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num\n",
    "\n",
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "\n",
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(pooled_embed)\n",
    "\n",
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "\n",
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)\n",
    "\n",
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# 7. 编译并训练模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# 构建模型输入字典\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_len)\n",
    "\n",
    "# 模型训练\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4c309-8eb3-489e-b655-94ecad0bfbd8",
   "metadata": {},
   "source": [
    "### 3.详细注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39765f5-5f8b-437d-8dd3-9b50f9b92bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>genres_bak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3299</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>968035345</td>\n",
       "      <td>Ed Wood (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>19119</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3630</td>\n",
       "      <td>3256</td>\n",
       "      <td>3</td>\n",
       "      <td>966536874</td>\n",
       "      <td>Patriot Games (1992)</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>77005</td>\n",
       "      <td>Action|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>976203603</td>\n",
       "      <td>Bridges of Madison County, The (1995)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>55408</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>785</td>\n",
       "      <td>2115</td>\n",
       "      <td>3</td>\n",
       "      <td>975430389</td>\n",
       "      <td>Indiana Jones and the Temple of Doom (1984)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>29307</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5848</td>\n",
       "      <td>909</td>\n",
       "      <td>5</td>\n",
       "      <td>957782527</td>\n",
       "      <td>Apartment, The (1960)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>20009</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0     3299       235       4  968035345   \n",
       "1     3630      3256       3  966536874   \n",
       "2      517       105       4  976203603   \n",
       "3      785      2115       3  975430389   \n",
       "4     5848       909       5  957782527   \n",
       "\n",
       "                                         title            genres gender  age  \\\n",
       "0                               Ed Wood (1994)      Comedy|Drama      F   25   \n",
       "1                         Patriot Games (1992)   Action|Thriller      M   18   \n",
       "2        Bridges of Madison County, The (1995)     Drama|Romance      F   25   \n",
       "3  Indiana Jones and the Temple of Doom (1984)  Action|Adventure      M   18   \n",
       "4                        Apartment, The (1960)      Comedy|Drama      M   50   \n",
       "\n",
       "   occupation    zip        genres_bak  \n",
       "0           4  19119      Comedy|Drama  \n",
       "1           4  77005   Action|Thriller  \n",
       "2          14  55408     Drama|Romance  \n",
       "3          19  29307  Action|Adventure  \n",
       "4          20  20009      Comedy|Drama  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50a39bb-8a3f-4f91-a801-745f3705c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae6578a-034a-4dc5-a905-aececfa9c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e804ee24-a602-47ac-b296-448d946b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9329ef-f738-40ae-9d8d-dd9b477d92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92368043-47cc-4c30-8af6-c7e4db4f76a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a745c17-2d69-4ea0-8953-cffde01d7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': <keras_preprocessing.text.Tokenizer at 0x2d809039eb0>,\n",
       " 'genres_bak': <keras_preprocessing.text.Tokenizer at 0x2d809039f40>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94580812-7dfa-49f7-a126-398e8472fe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': 6, 'genres_bak': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e57c9e-5da6-48d6-8937-17de54c3c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe7bd947-80bb-4bab-928a-f737e887004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': 187,\n",
       " 'user_id': 193,\n",
       " 'gender': 2,\n",
       " 'age': 7,\n",
       " 'occupation': 20,\n",
       " 'zip': 188,\n",
       " 'genres': 21,\n",
       " 'genres_bak': 21}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1750201-5824-44d7-a523-d67c02ca7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3143e25b-acc5-47a6-9d19-d4fab11b1d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <keras.layers.embeddings.Embedding at 0x2d809024e20>,\n",
       " 'user_id': <keras.layers.embeddings.Embedding at 0x2d7dcc67bb0>,\n",
       " 'gender': <keras.layers.embeddings.Embedding at 0x2d80a0a04c0>,\n",
       " 'age': <keras.layers.embeddings.Embedding at 0x2d8090394c0>,\n",
       " 'occupation': <keras.layers.embeddings.Embedding at 0x2d809039460>,\n",
       " 'zip': <keras.layers.embeddings.Embedding at 0x2d809039a00>,\n",
       " 'genres': <keras.layers.embeddings.Embedding at 0x2d806ae9460>,\n",
       " 'genres_bak': <keras.layers.embeddings.Embedding at 0x2d8090392b0>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e9383e-7c44-4970-a85d-6562810e9b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eedf480-5f59-4b65-82c7-9d9563799a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) 从这可以看出边长序列的字段数据最终整体也是当成一个字段处理\n",
    "    embeds.append(pooled_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd503f56-a6b5-44dc-843f-bea013606eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_16')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_17')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_18')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_19')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_20')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_21')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_4')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_5')>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "487d73ea-32a2-4df1-a2ff-1d64fc8eba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 4) dtype=float32 (created by layer 'tf.concat_3')>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "total_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef2679a8-b53b-477e-9f7d-860a8324dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13fe9dd5-2113-4ead-bbd3-3f5fb657a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29622049-2cbb-42c3-80c3-5cee47fe99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'add_3')>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbcdf14a-64cd-4c7d-9718-0eeae6fb1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b665b19-2dae-4260-b92b-d458d496f70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2d80c3f4190>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e303ef3-a2af-437d-b54c-ebc290d5cc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "811b4a3c-b842-4100-aa02-0b287bc1a40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e049473-510d-4c86-bcae-22b9c1ffbe1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}