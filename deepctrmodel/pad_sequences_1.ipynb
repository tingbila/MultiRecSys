{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa67662-0019-4f4c-ae07-4a5c444c9a8b",
   "metadata": {},
   "source": [
    "### 1. å˜é•¿åºåˆ—çš„å¤„ç†æ–¹å¼ç†è§£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124c136-7560-4a0d-bade-d3e65e5a1f13",
   "metadata": {},
   "source": [
    "å˜é•¿åºåˆ—çš„å¤„ç†é€»è¾‘å°±æ˜¯ï¼š\n",
    "\n",
    "ğŸ‘‰ è™½ç„¶åŸå§‹çš„å˜é•¿åºåˆ—ï¼ˆå¦‚ genresã€keywordsï¼‰åœ¨è¾“å…¥æ—¶æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œä½†åœ¨ç»è¿‡ embedding å’Œ poolingï¼ˆè¿™é‡Œæ˜¯ mean poolingï¼‰ä¹‹åï¼Œä¼šè¢«å‹ç¼©æˆä¸€ä¸ªå®šé•¿å‘é‡ï¼Œç›¸å½“äºä¸€ä¸ªâ€œæ•´ä½“è¯­ä¹‰â€è¡¨ç¤ºã€‚\n",
    "æ¢å¥è¯è¯´ï¼š\n",
    "\n",
    "åŸå§‹åºåˆ—ï¼ˆå¦‚ ['åŠ¨ä½œ', 'å†’é™©', 'ç§‘å¹»']ï¼‰\n",
    "â¬‡ï¸\n",
    "é€šè¿‡ Embedding â†’ å˜æˆ (batch_size, seq_len, embedding_dim) çš„å¼ é‡\n",
    "â¬‡ï¸\n",
    "é€šè¿‡ mean pooling â†’ æ±‡æ€»æˆ (batch_size, 1, embedding_dim)\n",
    "â¬‡ï¸\n",
    "å’Œå…¶å®ƒç¨€ç–ç‰¹å¾ä¸€èµ·å‚ä¸æ‹¼æ¥ã€å»ºæ¨¡\n",
    "æ‰€ä»¥æœ€ç»ˆï¼š\n",
    "\n",
    "æ— è®ºæ˜¯ç¨€ç–ç‰¹å¾ï¼ˆå•ä¸ªç´¢å¼•ï¼‰è¿˜æ˜¯å˜é•¿ç‰¹å¾ï¼ˆåºåˆ—ï¼‰ï¼Œæœ€ç»ˆéƒ½ç»Ÿä¸€æˆ (batch_size, 1, embedding_dim) çš„å½¢å¼ï¼Œè¿™æ ·æ‰èƒ½æ‹¼æ¥èµ·æ¥å–‚ç»™ FM å’Œ DNNã€‚\n",
    "\n",
    "è¿™ç§åšæ³•å…¶å®å°±æ˜¯â€œæŠŠå˜é•¿ç‰¹å¾è½¬åŒ–ä¸ºå®šé•¿ç‰¹å¾â€çš„é€šç”¨å¥—è·¯ä¹‹ä¸€ï¼ˆè¿˜æœ‰æ¯”å¦‚ attention poolingã€CNNã€RNN éƒ½å¯ä»¥åšç±»ä¼¼äº‹æƒ…ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34f13-7086-4c45-91bd-2e114dc9aa10",
   "metadata": {},
   "source": [
    "### 2. æ•´ä½“ç¨‹åº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d75a1f-a740-4b19-ab6e-e8a3314bb925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14.1148 - mse: 14.1148 - val_loss: 13.2294 - val_mse: 13.2294\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.0121 - mse: 14.0121 - val_loss: 13.1344 - val_mse: 13.1344\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.9067 - mse: 13.9067 - val_loss: 13.0361 - val_mse: 13.0361\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.7975 - mse: 13.7975 - val_loss: 12.9349 - val_mse: 12.9349\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6829 - mse: 13.6829 - val_loss: 12.8296 - val_mse: 12.8296\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.5639 - mse: 13.5639 - val_loss: 12.7197 - val_mse: 12.7197\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.4389 - mse: 13.4389 - val_loss: 12.6029 - val_mse: 12.6029\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.3079 - mse: 13.3079 - val_loss: 12.4807 - val_mse: 12.4807\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.1705 - mse: 13.1705 - val_loss: 12.3518 - val_mse: 12.3518\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.0259 - mse: 13.0259 - val_loss: 12.2161 - val_mse: 12.2161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d807caaca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 1. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®ï¼Œè¿™é‡Œæ¨¡æ‹Ÿå‡º2ä¸ªå˜é•¿åºåˆ—æ•°æ®\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]\n",
    "\n",
    "# å¯¹ç¨€ç–ç‰¹å¾åšæ ‡ç­¾ç¼–ç ï¼ˆLabel Encodingï¼‰\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "# ç”¨äºå­˜å‚¨æ¯ä¸ªå˜é•¿ç‰¹å¾å¤„ç†åçš„ padding åºåˆ—\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# æ¯ä¸ªå˜é•¿ç‰¹å¾å¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„ Tokenizerï¼Œç”¨äºåç»­æ–‡æœ¬è½¬ç´¢å¼•\n",
    "tokenizers = {}\n",
    "\n",
    "# ç”¨äºè®°å½•æ¯ä¸ªå˜é•¿ç‰¹å¾çš„ padding é•¿åº¦ï¼ˆå³åºåˆ—è¢«å¡«å……åçš„æœ€å¤§é•¿åº¦ï¼‰\n",
    "pad_len_dict = {}\n",
    "\n",
    "# éå†æ‰€æœ‰å˜é•¿åºåˆ—ç‰¹å¾\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # ä¿å­˜æ¯ä¸ªç‰¹å¾çš„åºåˆ—é•¿åº¦ï¼ˆmax_seq_lenï¼‰\n",
    "\n",
    "# 2. åˆ›å»ºæ‰€æœ‰ç‰¹å¾çš„Embeddingå±‚\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num\n",
    "\n",
    "# åˆ›å»ºåµŒå…¥å±‚å­—å…¸\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "# 1. åˆå§‹åŒ–æ¨¡å‹è¾“å…¥å­—å…¸\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "\n",
    "# 2. æ„å»ºåµŒå…¥åˆ—è¡¨\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) ä»è¿™å¯ä»¥çœ‹å‡ºè¾¹é•¿åºåˆ—çš„å­—æ®µæ•°æ®æœ€ç»ˆæ•´ä½“ä¹Ÿæ˜¯å½“æˆä¸€ä¸ªå­—æ®µå¤„ç†\n",
    "    embeds.append(pooled_embed)\n",
    "\n",
    "# æ‹¼æ¥æ‰€æœ‰åµŒå…¥ç‰¹å¾\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "\n",
    "# 4. FM äºŒé˜¶äº¤å‰é¡¹è®¡ç®—\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN éƒ¨åˆ†\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)\n",
    "\n",
    "# 6. åˆå¹¶ FM å’Œ DNN è¾“å‡ºç»“æœ\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# 7. ç¼–è¯‘å¹¶è®­ç»ƒæ¨¡å‹\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# æ„å»ºæ¨¡å‹è¾“å…¥å­—å…¸\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_len)\n",
    "\n",
    "# æ¨¡å‹è®­ç»ƒ\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41b66504-42d1-4988-8530-d32cb2ba9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a97de804-a21e-4bd9-816c-6b2b92a8ee5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>genres_bak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3299</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>968035345</td>\n",
       "      <td>Ed Wood (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>19119</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3630</td>\n",
       "      <td>3256</td>\n",
       "      <td>3</td>\n",
       "      <td>966536874</td>\n",
       "      <td>Patriot Games (1992)</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>77005</td>\n",
       "      <td>Action|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>976203603</td>\n",
       "      <td>Bridges of Madison County, The (1995)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>55408</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>785</td>\n",
       "      <td>2115</td>\n",
       "      <td>3</td>\n",
       "      <td>975430389</td>\n",
       "      <td>Indiana Jones and the Temple of Doom (1984)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>29307</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5848</td>\n",
       "      <td>909</td>\n",
       "      <td>5</td>\n",
       "      <td>957782527</td>\n",
       "      <td>Apartment, The (1960)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>20009</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0     3299       235       4  968035345   \n",
       "1     3630      3256       3  966536874   \n",
       "2      517       105       4  976203603   \n",
       "3      785      2115       3  975430389   \n",
       "4     5848       909       5  957782527   \n",
       "\n",
       "                                         title            genres gender  age  \\\n",
       "0                               Ed Wood (1994)      Comedy|Drama      F   25   \n",
       "1                         Patriot Games (1992)   Action|Thriller      M   18   \n",
       "2        Bridges of Madison County, The (1995)     Drama|Romance      F   25   \n",
       "3  Indiana Jones and the Temple of Doom (1984)  Action|Adventure      M   18   \n",
       "4                        Apartment, The (1960)      Comedy|Drama      M   50   \n",
       "\n",
       "   occupation    zip        genres_bak  \n",
       "0           4  19119      Comedy|Drama  \n",
       "1           4  77005   Action|Thriller  \n",
       "2          14  55408     Drama|Romance  \n",
       "3          19  29307  Action|Adventure  \n",
       "4          20  20009      Comedy|Drama  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®ï¼Œè¿™é‡Œæ¨¡æ‹Ÿå‡º2ä¸ªå˜é•¿åºåˆ—æ•°æ®\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cee59f0-eaba-4ec8-82c3-0b81fc652e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c47d80dd-67df-480b-8773-cf423bdecd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹ç¨€ç–ç‰¹å¾åšæ ‡ç­¾ç¼–ç ï¼ˆLabel Encodingï¼‰\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a36731e9-75b4-4641-b0af-f14ff1606638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨äºå­˜å‚¨æ¯ä¸ªå˜é•¿ç‰¹å¾å¤„ç†åçš„ padding åºåˆ—\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# æ¯ä¸ªå˜é•¿ç‰¹å¾å¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„ Tokenizerï¼Œç”¨äºåç»­æ–‡æœ¬è½¬ç´¢å¼•\n",
    "tokenizers = {}\n",
    "\n",
    "# ç”¨äºè®°å½•æ¯ä¸ªå˜é•¿ç‰¹å¾çš„ padding é•¿åº¦ï¼ˆå³åºåˆ—è¢«å¡«å……åçš„æœ€å¤§é•¿åº¦ï¼‰\n",
    "pad_len_dict = {}\n",
    "\n",
    "# éå†æ‰€æœ‰å˜é•¿åºåˆ—ç‰¹å¾\n",
    "for feature in sequence_features:\n",
    "    # ç¬¬ä¸€æ­¥ï¼šå°†åŸå§‹æ•°æ®ä¸­ç”¨ '|' åˆ†éš”çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ 'a|b|c'ï¼‰è½¬æ¢ä¸º 'a b c'\n",
    "    # è¿™æ˜¯ä¸ºäº†è®© Tokenizer æ­£å¸¸å°†å…¶è§†ä¸ºä¸€ä¸ªè¯åºåˆ—\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    \n",
    "    # ç¬¬äºŒæ­¥ï¼šåˆ›å»º Tokenizerï¼Œç”¨äºå°†è¯æ˜ å°„ä¸ºç´¢å¼•\n",
    "    # è®¾ç½® oov_token='OOV' å¯ä»¥è®©æœªç™»å½•è¯æ˜ å°„ä¸ºåŒä¸€ä¸ªç´¢å¼•ï¼Œé¿å…æŠ¥é”™\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    \n",
    "    # ç¬¬ä¸‰æ­¥ï¼šæ‹Ÿåˆ Tokenizerï¼Œç»Ÿè®¡è¯é¢‘å¹¶å»ºç«‹è¯åˆ°ç´¢å¼•çš„æ˜ å°„è¡¨\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # ç¬¬å››æ­¥ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—ï¼Œä¾‹å¦‚ ['a b c'] -> [[1, 5, 9]]\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # ç¬¬äº”æ­¥ï¼šå°†ç´¢å¼•åºåˆ—è¿›è¡Œ paddingï¼Œä½¿æ‰€æœ‰åºåˆ—ç­‰é•¿ï¼ˆå³ä¾§å¡«å……ï¼‰\n",
    "    # å¡«å……é•¿åº¦è‡ªåŠ¨ç­‰äºè¯¥ç‰¹å¾æ‰€æœ‰æ ·æœ¬ä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦\n",
    "    padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    # ç¬¬å…­æ­¥ï¼šå­˜å‚¨ç»“æœåˆ°å­—å…¸ä¸­ï¼Œä¾¿äºåç»­æ¨¡å‹ä½¿ç”¨\n",
    "    pad_sequences_dict[feature] = padded            # å­˜å‚¨å¡«å……åçš„åºåˆ—\n",
    "    tokenizers[feature] = tokenizer                 # å­˜å‚¨è¯¥ç‰¹å¾çš„ tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]         # å­˜å‚¨è¯¥ç‰¹å¾çš„ padding é•¿åº¦ï¼ˆåˆ—æ•°ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da768ffb-ae03-45ce-8d3b-d047f8e34ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d03be4d3-ce0b-459e-ad4c-4054fdbe014d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b07b756c-4c1b-4147-bd32-d3f201ba688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': <keras_preprocessing.text.Tokenizer at 0x2870704d580>,\n",
       " 'genres_bak': <keras_preprocessing.text.Tokenizer at 0x2870704d3a0>}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "102e1ed0-07da-43cf-900d-811e17b63e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OOV': 1,\n",
       " 'comedy': 2,\n",
       " 'drama': 3,\n",
       " 'action': 4,\n",
       " 'thriller': 5,\n",
       " 'romance': 6,\n",
       " 'sci': 7,\n",
       " 'fi': 8,\n",
       " 'adventure': 9,\n",
       " 'horror': 10,\n",
       " 'crime': 11,\n",
       " \"children's\": 12,\n",
       " 'fantasy': 13,\n",
       " 'war': 14,\n",
       " 'western': 15,\n",
       " 'mystery': 16,\n",
       " 'musical': 17,\n",
       " 'animation': 18,\n",
       " 'film': 19,\n",
       " 'noir': 20}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers['genres'].word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef5b5e67-3bb8-4453-8b7f-245e182f6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. åˆ›å»ºæ‰€æœ‰ç‰¹å¾çš„Embeddingå±‚\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08b34ed6-a780-4050-b2ec-1471471d38ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': 187,\n",
       " 'user_id': 193,\n",
       " 'gender': 2,\n",
       " 'age': 7,\n",
       " 'occupation': 20,\n",
       " 'zip': 188}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0fffedd-11f6-4a89-988f-e9e342387f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x2870704d580>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28b6a06e-ff58-4994-a1df-a6939dc3b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1  # +1 æ˜¯å› ä¸º index ä» 1 å¼€å§‹ï¼Œ0 æ˜¯ padding\n",
    "    vocab_sizes[feature] = feat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5c06a5b-0007-4119-9fae-2a91b3fe0d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': 187,\n",
       " 'user_id': 193,\n",
       " 'gender': 2,\n",
       " 'age': 7,\n",
       " 'occupation': 20,\n",
       " 'zip': 188,\n",
       " 'genres': 21,\n",
       " 'genres_bak': 21}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebd2a089-76d0-4e38-b1a8-66904d3b746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåµŒå…¥å±‚å­—å…¸\n",
    "embed_layers = {}\n",
    "# æ™®é€šç¨€ç–ç‰¹å¾ä¸ä½¿ç”¨ mask_zero\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1,output_dim=embedding_dim,mask_zero=False)\n",
    "\n",
    "# å˜é•¿åºåˆ—ç‰¹å¾ä½¿ç”¨ mask_zero=Trueï¼Œå¿½ç•¥ padding ä½ç½®\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1,output_dim=embedding_dim,mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d25625a-22d5-494d-8761-e07d698f8056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <keras.layers.embeddings.Embedding at 0x2870704de50>,\n",
       " 'user_id': <keras.layers.embeddings.Embedding at 0x2870704ddc0>,\n",
       " 'gender': <keras.layers.embeddings.Embedding at 0x2870704db80>,\n",
       " 'age': <keras.layers.embeddings.Embedding at 0x2870706c250>,\n",
       " 'occupation': <keras.layers.embeddings.Embedding at 0x2870706c4f0>,\n",
       " 'zip': <keras.layers.embeddings.Embedding at 0x2870706c730>,\n",
       " 'genres': <keras.layers.embeddings.Embedding at 0x2870704df70>,\n",
       " 'genres_bak': <keras.layers.embeddings.Embedding at 0x2870706c9a0>}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cf655-2f00-4830-9ea9-c01636bf6a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf6d946f-d15a-43d4-a0e8-06975262a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. åˆå§‹åŒ–æ¨¡å‹è¾“å…¥å­—å…¸\n",
    "inputs = {}\n",
    "\n",
    "# 2. æ·»åŠ ç¨€ç–ç‰¹å¾ï¼ˆé€šå¸¸æ˜¯å•ä¸ª ID æˆ–ç±»åˆ«ç´¢å¼•ï¼‰\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)\n",
    "\n",
    "# 3. æ·»åŠ å˜é•¿åºåˆ—ç‰¹å¾ï¼Œä½¿ç”¨ pad_len_dict è‡ªåŠ¨è·å–æ¯ä¸ªç‰¹å¾çš„ padding é•¿åº¦\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]  # è·å–å½“å‰åºåˆ—ç‰¹å¾çš„æœ€å¤§é•¿åº¦\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "83abb9a7-4bfc-40c1-af82-985f79d7838d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f47b419d-ff5a-45bd-8152-19411313e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç¨€ç–ç‰¹å¾åµŒå…¥ï¼šshape -> (batch_size, 1, embedding_dim)\n",
    "embeds = [embed_layers[feat](inputs[feat]) for feat in sparse_features]\n",
    "\n",
    "# 2. å˜é•¿åºåˆ—ç‰¹å¾åµŒå…¥ + mean poolingï¼šshape -> (batch_size, 1, embedding_dim)\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # mean pooling -> (batch_size, 1, embedding_dim)\n",
    "    embeds.append(pooled_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dec18cb8-f289-497b-8c9a-afdc22ca0069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_16')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_17')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_18')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_19')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_20')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_21')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_1')>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "857d19e7-54dd-4865-969e-4ef9d7ebfb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 4) dtype=float32 (created by layer 'tf.concat_1')>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ‹¼æ¥æ‰€æœ‰åµŒå…¥ç‰¹å¾ï¼ˆåŒ…æ‹¬ç¨€ç–ç‰¹å¾ + å˜é•¿ç‰¹å¾çš„mean poolingç»“æœï¼‰\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "total_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37cf9fa5-b8f2-4c8b-95eb-a1ef64fdca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FM äºŒé˜¶äº¤å‰é¡¹è®¡ç®—\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))       # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)      # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN éƒ¨åˆ† - flatten å±•å¹³åé€å…¥å¤šå±‚å…¨è¿æ¥å±‚\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "dnn_output = layers.Dense(1)(x)  # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be94c50e-42e0-4d0b-ac94-5a26e9c129c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'tf.math.multiply')>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "286bcc1f-338c-4a03-9759-27c81ca43327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_2')>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "175272d3-5acf-40c9-9084-929f6ec60b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d09c3b4e-16f8-436a-b116-492b51de474f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ba9c3ee-b39b-464d-a367-022e8deee779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inputs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52d23a07-b0cf-4019-baff-dd21350571b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. åˆå¹¶ FM å’Œ DNN è¾“å‡ºç»“æœ\n",
    "output = layers.Add()([fm_second_order, dnn_output])\n",
    "output # model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1c7a48cd-6685-43c9-a5eb-6b2c7eb6d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ç¼–è¯‘å¹¶è®­ç»ƒæ¨¡å‹\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d85bf62d-4a86-4b7b-a804-e6dfcfeddcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹è¾“å…¥å­—å…¸\n",
    "model_input = {}\n",
    "\n",
    "# æ·»åŠ ç¨€ç–ç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values\n",
    "\n",
    "# æ·»åŠ å˜é•¿åºåˆ—ç‰¹å¾\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f8971a3-313c-4f95-ad0e-4a7941737f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': array([ 12, 169,   6, 112,  45, 146,  43, 156,  30, 174,  82, 173,  91,\n",
       "        108, 132,  40, 109,  31, 180, 183, 129,  67, 137,  87, 127,   8,\n",
       "        104, 100, 140,  25, 122, 124, 116, 126,  72, 117,  42, 145, 131,\n",
       "          2,  52,  17, 101,  94, 136,  65,  20, 144,  26,  83,  55, 126,\n",
       "        184,  23, 121, 142,  33,   0,  46, 139, 150, 135,  36, 110,  79,\n",
       "        162,  70, 147,   9,  34,   7,  76,   4, 185,  73, 112, 130,  95,\n",
       "         28,  24, 148, 119, 168, 149, 181,  13, 154,  56,  66, 172,  69,\n",
       "         35,  49, 106,  35,  11, 152, 166,  37, 164,  54, 167,  72,  29,\n",
       "         92, 114,  88, 170,  64,  60,  38,  22,  62, 178, 134, 157,  99,\n",
       "         34, 111,  96,  50,  75,  47,  14,  21,  77, 118, 182, 113, 143,\n",
       "        149, 141,  10,  58,  81,  44,  27, 151, 165,  98, 163,  80, 158,\n",
       "        161,  27, 155, 171,  78,  57, 123,  84,  93, 170, 120,   1, 153,\n",
       "         39,  61,  51,  71,  19, 107,   9,  66, 102,  74, 177, 103, 133,\n",
       "        160,  53,  90,   5, 173,  41,  59, 123, 159,  48, 115, 138,  63,\n",
       "         16, 179,   3,  97, 128, 186, 175, 105, 169,  32,  68,  18,  85,\n",
       "        176,  89, 125,  15,  86], dtype=int64),\n",
       " 'user_id': array([107, 123,  12,  21, 187,  99, 102,  24, 134,  68,  97,   7,  77,\n",
       "          1, 125,  78, 191, 124, 145,  99, 186,  69, 175,  44, 103, 150,\n",
       "         54,  73, 113,   9,  65, 159, 144,  37,  86, 176,  16, 100,  75,\n",
       "         14,   6, 165, 143, 177,  66,  53,  20,  64, 179,  52, 152,  94,\n",
       "         61,  93,  62,   5,  91,  26,  81,  83,  29,  50,  58,  88,  57,\n",
       "         44,  72, 117, 184, 172, 185,  31, 130,  28, 166, 182, 146,  70,\n",
       "        139,  43,  82,  87, 192, 142, 167,  15,  92,  85, 183, 168, 111,\n",
       "          0,  76,  80, 133,  13,  36, 136,  95, 135,  33, 174,  90, 108,\n",
       "        162, 148, 118,  11, 119,  98,  19, 190, 155,  32, 112,  38,  56,\n",
       "        157, 160, 138, 173,   8, 149,  23,  96,  63,  42, 106, 188, 114,\n",
       "        109, 163, 170, 140,  29,  22, 189, 153, 156,  35, 141, 171,  49,\n",
       "         89,  39, 105, 122, 129,  30, 147, 104,  40, 178,  47, 180, 127,\n",
       "          3,  40,  79,  17,  27,  59,  45, 154,  48, 101, 115,  18, 116,\n",
       "        132,   2,  60, 120,  10,  34, 169,  41, 164,  74, 128,  55, 151,\n",
       "         51, 137,  71, 173,  49,  67, 138, 121, 110, 126, 161, 158,  84,\n",
       "         46, 131,   4, 181,  25], dtype=int64),\n",
       " 'gender': array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "        1, 1]),\n",
       " 'age': array([2, 1, 2, 1, 5, 1, 0, 5, 2, 5, 3, 2, 1, 6, 2, 1, 2, 3, 3, 1, 2, 4,\n",
       "        4, 3, 2, 6, 2, 6, 4, 2, 1, 3, 2, 1, 5, 4, 1, 4, 3, 3, 5, 2, 5, 2,\n",
       "        5, 5, 0, 2, 6, 2, 3, 1, 2, 3, 3, 2, 1, 1, 4, 5, 2, 2, 1, 1, 5, 3,\n",
       "        3, 3, 2, 6, 2, 4, 2, 3, 2, 2, 6, 4, 5, 2, 4, 2, 2, 2, 1, 5, 5, 3,\n",
       "        1, 3, 3, 3, 5, 2, 2, 2, 1, 2, 5, 2, 1, 2, 3, 1, 3, 2, 1, 2, 2, 3,\n",
       "        1, 1, 6, 1, 3, 3, 2, 1, 5, 3, 1, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 4,\n",
       "        2, 2, 2, 0, 0, 2, 5, 2, 5, 2, 2, 2, 1, 5, 6, 5, 3, 5, 2, 3, 2, 2,\n",
       "        3, 0, 1, 3, 5, 6, 5, 2, 2, 6, 2, 2, 1, 2, 4, 2, 1, 1, 4, 3, 2, 1,\n",
       "        2, 5, 1, 3, 3, 2, 5, 2, 6, 1, 2, 2, 3, 1, 3, 4, 6, 3, 0, 2, 1, 1,\n",
       "        2, 2], dtype=int64),\n",
       " 'occupation': array([ 4,  4, 13, 18, 19,  0,  1,  1, 16, 10, 19,  2,  4,  1,  4,  0, 14,\n",
       "         7, 16,  0,  1, 13,  7, 14,  2,  7, 19, 12,  1, 11,  0,  0,  4, 19,\n",
       "         1,  1,  4,  1,  3, 19, 14,  8,  0, 16, 15, 15,  9,  0, 15, 11,  5,\n",
       "         0,  0,  0,  5, 11, 16, 13,  6, 13,  0,  0,  4,  4,  6, 14, 16,  2,\n",
       "         2,  0,  0,  8,  7, 18,  7, 10,  1, 19,  7,  7,  1,  3,  6,  0, 13,\n",
       "        16, 19,  1, 14,  1, 19,  7, 15,  0,  0,  0,  4,  0, 11, 14,  4, 10,\n",
       "        10,  4,  4,  1, 16, 15,  0, 11,  4,  5,  1,  4,  1,  2, 19,  4, 13,\n",
       "        15, 11,  0,  4, 15,  0, 18, 13, 19,  7,  0, 11,  1, 15, 16,  0, 18,\n",
       "         9, 13, 17, 18, 11, 11,  0, 13,  1, 12, 16,  6,  3,  6, 17,  4,  6,\n",
       "         7, 16,  9,  1,  4, 11, 16,  0,  3,  0,  1, 11,  3,  4,  0,  7, 19,\n",
       "         4,  4,  0,  7,  3,  9, 18, 11,  4,  6, 11,  2,  7, 18, 12, 11,  0,\n",
       "         7, 15, 14,  7,  7,  2, 16,  9, 11, 11, 13,  0,  0], dtype=int64),\n",
       " 'zip': array([ 35, 118,  99,  55,  41, 108, 137,  45,  84, 144, 178, 140,  31,\n",
       "         86, 112,  94,  62,  16,  39, 108, 142, 165,   3,  44, 135,  70,\n",
       "        166,   9,  33,  96, 162, 155,  79,  85,  69, 145, 149, 136, 187,\n",
       "         23,  95, 150,   1,   2, 143, 184,  72,  19,  61, 123, 172,  88,\n",
       "         78,  10,  25, 139,  90, 117,   4,  32,  21, 164,   0, 152,  60,\n",
       "         44, 157, 107, 115, 119, 146, 169, 102, 159, 103,   6,  76, 148,\n",
       "         71,  40, 105,  87,  26,  73,  20,  64,  54,  50, 153, 114,  59,\n",
       "        100,  15,  77, 127,  68, 181,  42, 177, 126, 168,  99,  80, 130,\n",
       "        174, 141, 111,  93, 183, 171, 180, 175,  71, 132,  43,  36,  37,\n",
       "        109,  98,  75, 134,  97, 186,  74,  22,  29,  66,   5,  11, 183,\n",
       "        120,  34,  58, 185,  21,  89,  46,  52,  14, 128,  63, 161, 104,\n",
       "         38, 158, 154,  49,  30,  27,  81,  47, 179,   8, 133,  17,  12,\n",
       "         24, 179, 125,  65, 182, 173, 131,  51, 147,  92, 122,  67,  57,\n",
       "        138,  13, 118, 121,  18, 124, 110, 156, 167, 101,  91, 170, 176,\n",
       "         53,  28, 163, 134, 104, 151,  75,   7, 116,  56, 160, 129,  82,\n",
       "         48, 113,  83, 106, 136]),\n",
       " 'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ae67852-153b-45db-8a36-bd98c47cd1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14.3986 - mse: 14.3986 - val_loss: 13.5323 - val_mse: 13.5323\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 14.3149 - mse: 14.3149 - val_loss: 13.4644 - val_mse: 13.4644\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 14.2341 - mse: 14.2341 - val_loss: 13.3972 - val_mse: 13.3972\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 14.1548 - mse: 14.1548 - val_loss: 13.3303 - val_mse: 13.3303\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 14.0771 - mse: 14.0771 - val_loss: 13.2637 - val_mse: 13.2637\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.9997 - mse: 13.9997 - val_loss: 13.1960 - val_mse: 13.1960\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.9211 - mse: 13.9211 - val_loss: 13.1265 - val_mse: 13.1265\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.8410 - mse: 13.8410 - val_loss: 13.0557 - val_mse: 13.0557\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 13.7583 - mse: 13.7583 - val_loss: 12.9832 - val_mse: 12.9832\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6733 - mse: 13.6733 - val_loss: 12.9097 - val_mse: 12.9097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2870824cbe0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ¨¡å‹è®­ç»ƒ\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc5e8d33-179a-4e53-a6fb-3c04d6105d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 14.3441 - mse: 14.3441 - val_loss: 13.4779 - val_mse: 13.4779\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.2641 - mse: 14.2641 - val_loss: 13.4092 - val_mse: 13.4092\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.1868 - mse: 14.1868 - val_loss: 13.3433 - val_mse: 13.3433\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 14.1119 - mse: 14.1119 - val_loss: 13.2792 - val_mse: 13.2792\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.0390 - mse: 14.0390 - val_loss: 13.2122 - val_mse: 13.2122\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.9657 - mse: 13.9657 - val_loss: 13.1433 - val_mse: 13.1433\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 13.8919 - mse: 13.8919 - val_loss: 13.0734 - val_mse: 13.0734\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.8160 - mse: 13.8160 - val_loss: 13.0024 - val_mse: 13.0024\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.7372 - mse: 13.7372 - val_loss: 12.9296 - val_mse: 12.9296\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6554 - mse: 13.6554 - val_loss: 12.8539 - val_mse: 12.8539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2870a685910>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 1. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®ï¼Œè¿™é‡Œæ¨¡æ‹Ÿå‡º2ä¸ªå˜é•¿åºåˆ—æ•°æ®\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]\n",
    "\n",
    "# å¯¹ç¨€ç–ç‰¹å¾åšæ ‡ç­¾ç¼–ç ï¼ˆLabel Encodingï¼‰\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "# ç”¨äºå­˜å‚¨æ¯ä¸ªå˜é•¿ç‰¹å¾å¤„ç†åçš„ padding åºåˆ—\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# æ¯ä¸ªå˜é•¿ç‰¹å¾å¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„ Tokenizerï¼Œç”¨äºåç»­æ–‡æœ¬è½¬ç´¢å¼•\n",
    "tokenizers = {}\n",
    "\n",
    "# ç”¨äºè®°å½•æ¯ä¸ªå˜é•¿ç‰¹å¾çš„ padding é•¿åº¦ï¼ˆå³åºåˆ—è¢«å¡«å……åçš„æœ€å¤§é•¿åº¦ï¼‰\n",
    "pad_len_dict = {}\n",
    "\n",
    "# éå†æ‰€æœ‰å˜é•¿åºåˆ—ç‰¹å¾\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # ä¿å­˜æ¯ä¸ªç‰¹å¾çš„åºåˆ—é•¿åº¦ï¼ˆmax_seq_lenï¼‰\n",
    "\n",
    "# 2. åˆ›å»ºæ‰€æœ‰ç‰¹å¾çš„Embeddingå±‚\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num\n",
    "\n",
    "# åˆ›å»ºåµŒå…¥å±‚å­—å…¸\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "# 1. åˆå§‹åŒ–æ¨¡å‹è¾“å…¥å­—å…¸\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "\n",
    "# 2. æ„å»ºåµŒå…¥åˆ—è¡¨\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(pooled_embed)\n",
    "\n",
    "# æ‹¼æ¥æ‰€æœ‰åµŒå…¥ç‰¹å¾\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "\n",
    "# 4. FM äºŒé˜¶äº¤å‰é¡¹è®¡ç®—\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN éƒ¨åˆ†\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)\n",
    "\n",
    "# 6. åˆå¹¶ FM å’Œ DNN è¾“å‡ºç»“æœ\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# 7. ç¼–è¯‘å¹¶è®­ç»ƒæ¨¡å‹\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# æ„å»ºæ¨¡å‹è¾“å…¥å­—å…¸\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_len)\n",
    "\n",
    "# æ¨¡å‹è®­ç»ƒ\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}