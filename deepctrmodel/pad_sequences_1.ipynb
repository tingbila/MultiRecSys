{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa67662-0019-4f4c-ae07-4a5c444c9a8b",
   "metadata": {},
   "source": [
    "### 1. 变长序列的处理方式理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124c136-7560-4a0d-bade-d3e65e5a1f13",
   "metadata": {},
   "source": [
    "变长序列的处理逻辑就是：\n",
    "\n",
    "👉 虽然原始的变长序列（如 genres、keywords）在输入时是一个序列，但在经过 embedding 和 pooling（这里是 mean pooling）之后，会被压缩成一个定长向量，相当于一个“整体语义”表示。\n",
    "换句话说：\n",
    "\n",
    "原始序列（如 ['动作', '冒险', '科幻']）\n",
    "⬇️\n",
    "通过 Embedding → 变成 (batch_size, seq_len, embedding_dim) 的张量\n",
    "⬇️\n",
    "通过 mean pooling → 汇总成 (batch_size, 1, embedding_dim)\n",
    "⬇️\n",
    "和其它稀疏特征一起参与拼接、建模\n",
    "所以最终：\n",
    "\n",
    "无论是稀疏特征（单个索引）还是变长特征（序列），最终都统一成 (batch_size, 1, embedding_dim) 的形式，这样才能拼接起来喂给 FM 和 DNN。\n",
    "\n",
    "这种做法其实就是“把变长特征转化为定长特征”的通用套路之一（还有比如 attention pooling、CNN、RNN 都可以做类似事情）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34f13-7086-4c45-91bd-2e114dc9aa10",
   "metadata": {},
   "source": [
    "### 2. 整体程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d75a1f-a740-4b19-ab6e-e8a3314bb925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14.1148 - mse: 14.1148 - val_loss: 13.2294 - val_mse: 13.2294\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.0121 - mse: 14.0121 - val_loss: 13.1344 - val_mse: 13.1344\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.9067 - mse: 13.9067 - val_loss: 13.0361 - val_mse: 13.0361\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.7975 - mse: 13.7975 - val_loss: 12.9349 - val_mse: 12.9349\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6829 - mse: 13.6829 - val_loss: 12.8296 - val_mse: 12.8296\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.5639 - mse: 13.5639 - val_loss: 12.7197 - val_mse: 12.7197\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.4389 - mse: 13.4389 - val_loss: 12.6029 - val_mse: 12.6029\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.3079 - mse: 13.3079 - val_loss: 12.4807 - val_mse: 12.4807\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.1705 - mse: 13.1705 - val_loss: 12.3518 - val_mse: 12.3518\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.0259 - mse: 13.0259 - val_loss: 12.2161 - val_mse: 12.2161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d807caaca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]\n",
    "\n",
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}\n",
    "\n",
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）\n",
    "\n",
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num\n",
    "\n",
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "\n",
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) 从这可以看出边长序列的字段数据最终整体也是当成一个字段处理\n",
    "    embeds.append(pooled_embed)\n",
    "\n",
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "\n",
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)\n",
    "\n",
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# 7. 编译并训练模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# 构建模型输入字典\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_len)\n",
    "\n",
    "# 模型训练\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4c309-8eb3-489e-b655-94ecad0bfbd8",
   "metadata": {},
   "source": [
    "### 3.详细注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e39765f5-5f8b-437d-8dd3-9b50f9b92bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>genres_bak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3299</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>968035345</td>\n",
       "      <td>Ed Wood (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>19119</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3630</td>\n",
       "      <td>3256</td>\n",
       "      <td>3</td>\n",
       "      <td>966536874</td>\n",
       "      <td>Patriot Games (1992)</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>77005</td>\n",
       "      <td>Action|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>976203603</td>\n",
       "      <td>Bridges of Madison County, The (1995)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>55408</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>785</td>\n",
       "      <td>2115</td>\n",
       "      <td>3</td>\n",
       "      <td>975430389</td>\n",
       "      <td>Indiana Jones and the Temple of Doom (1984)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>29307</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5848</td>\n",
       "      <td>909</td>\n",
       "      <td>5</td>\n",
       "      <td>957782527</td>\n",
       "      <td>Apartment, The (1960)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>20009</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0     3299       235       4  968035345   \n",
       "1     3630      3256       3  966536874   \n",
       "2      517       105       4  976203603   \n",
       "3      785      2115       3  975430389   \n",
       "4     5848       909       5  957782527   \n",
       "\n",
       "                                         title            genres gender  age  \\\n",
       "0                               Ed Wood (1994)      Comedy|Drama      F   25   \n",
       "1                         Patriot Games (1992)   Action|Thriller      M   18   \n",
       "2        Bridges of Madison County, The (1995)     Drama|Romance      F   25   \n",
       "3  Indiana Jones and the Temple of Doom (1984)  Action|Adventure      M   18   \n",
       "4                        Apartment, The (1960)      Comedy|Drama      M   50   \n",
       "\n",
       "   occupation    zip        genres_bak  \n",
       "0           4  19119      Comedy|Drama  \n",
       "1           4  77005   Action|Thriller  \n",
       "2          14  55408     Drama|Romance  \n",
       "3          19  29307  Action|Adventure  \n",
       "4          20  20009      Comedy|Drama  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c50a39bb-8a3f-4f91-a801-745f3705c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fae6578a-034a-4dc5-a905-aececfa9c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e804ee24-a602-47ac-b296-448d946b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b9329ef-f738-40ae-9d8d-dd9b477d92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92368043-47cc-4c30-8af6-c7e4db4f76a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a745c17-2d69-4ea0-8953-cffde01d7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': <keras_preprocessing.text.Tokenizer at 0x2d80c922a60>,\n",
       " 'genres_bak': <keras_preprocessing.text.Tokenizer at 0x2d7dcc67ac0>}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94580812-7dfa-49f7-a126-398e8472fe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': 6, 'genres_bak': 6}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1e57c9e-5da6-48d6-8937-17de54c3c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe7bd947-80bb-4bab-928a-f737e887004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': 187,\n",
       " 'user_id': 193,\n",
       " 'gender': 2,\n",
       " 'age': 7,\n",
       " 'occupation': 20,\n",
       " 'zip': 188,\n",
       " 'genres': 21,\n",
       " 'genres_bak': 21}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1750201-5824-44d7-a523-d67c02ca7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3143e25b-acc5-47a6-9d19-d4fab11b1d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <keras.layers.embeddings.Embedding at 0x2d80c922610>,\n",
       " 'user_id': <keras.layers.embeddings.Embedding at 0x2d80c922670>,\n",
       " 'gender': <keras.layers.embeddings.Embedding at 0x2d80c922700>,\n",
       " 'age': <keras.layers.embeddings.Embedding at 0x2d80c922f40>,\n",
       " 'occupation': <keras.layers.embeddings.Embedding at 0x2d80c9f2160>,\n",
       " 'zip': <keras.layers.embeddings.Embedding at 0x2d80c9f20d0>,\n",
       " 'genres': <keras.layers.embeddings.Embedding at 0x2d80c922c40>,\n",
       " 'genres_bak': <keras.layers.embeddings.Embedding at 0x2d80c9f28e0>}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35e9383e-7c44-4970-a85d-6562810e9b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eedf480-5f59-4b65-82c7-9d9563799a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) 从这可以看出边长序列的字段数据最终整体也是当成一个字段处理\n",
    "    embeds.append(pooled_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd503f56-a6b5-44dc-843f-bea013606eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_24')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_25')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_26')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_27')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_28')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_29')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_6')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_7')>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "487d73ea-32a2-4df1-a2ff-1d64fc8eba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 4) dtype=float32 (created by layer 'tf.concat_4')>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "total_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef2679a8-b53b-477e-9f7d-860a8324dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13fe9dd5-2113-4ead-bbd3-3f5fb657a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29622049-2cbb-42c3-80c3-5cee47fe99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'add_4')>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbcdf14a-64cd-4c7d-9718-0eeae6fb1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b665b19-2dae-4260-b92b-d458d496f70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2d80ca27a90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e303ef3-a2af-437d-b54c-ebc290d5cc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "811b4a3c-b842-4100-aa02-0b287bc1a40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e049473-510d-4c86-bcae-22b9c1ffbe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=list(inputs.values())\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b64757c7-ed96-4267-8449-231844931675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 编译并训练模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# 构建模型输入字典\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12f55d95-f94e-4f84-9c89-836a8ee3b9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': array([ 12, 169,   6, 112,  45, 146,  43, 156,  30, 174,  82, 173,  91,\n",
       "        108, 132,  40, 109,  31, 180, 183, 129,  67, 137,  87, 127,   8,\n",
       "        104, 100, 140,  25, 122, 124, 116, 126,  72, 117,  42, 145, 131,\n",
       "          2,  52,  17, 101,  94, 136,  65,  20, 144,  26,  83,  55, 126,\n",
       "        184,  23, 121, 142,  33,   0,  46, 139, 150, 135,  36, 110,  79,\n",
       "        162,  70, 147,   9,  34,   7,  76,   4, 185,  73, 112, 130,  95,\n",
       "         28,  24, 148, 119, 168, 149, 181,  13, 154,  56,  66, 172,  69,\n",
       "         35,  49, 106,  35,  11, 152, 166,  37, 164,  54, 167,  72,  29,\n",
       "         92, 114,  88, 170,  64,  60,  38,  22,  62, 178, 134, 157,  99,\n",
       "         34, 111,  96,  50,  75,  47,  14,  21,  77, 118, 182, 113, 143,\n",
       "        149, 141,  10,  58,  81,  44,  27, 151, 165,  98, 163,  80, 158,\n",
       "        161,  27, 155, 171,  78,  57, 123,  84,  93, 170, 120,   1, 153,\n",
       "         39,  61,  51,  71,  19, 107,   9,  66, 102,  74, 177, 103, 133,\n",
       "        160,  53,  90,   5, 173,  41,  59, 123, 159,  48, 115, 138,  63,\n",
       "         16, 179,   3,  97, 128, 186, 175, 105, 169,  32,  68,  18,  85,\n",
       "        176,  89, 125,  15,  86], dtype=int64),\n",
       " 'user_id': array([107, 123,  12,  21, 187,  99, 102,  24, 134,  68,  97,   7,  77,\n",
       "          1, 125,  78, 191, 124, 145,  99, 186,  69, 175,  44, 103, 150,\n",
       "         54,  73, 113,   9,  65, 159, 144,  37,  86, 176,  16, 100,  75,\n",
       "         14,   6, 165, 143, 177,  66,  53,  20,  64, 179,  52, 152,  94,\n",
       "         61,  93,  62,   5,  91,  26,  81,  83,  29,  50,  58,  88,  57,\n",
       "         44,  72, 117, 184, 172, 185,  31, 130,  28, 166, 182, 146,  70,\n",
       "        139,  43,  82,  87, 192, 142, 167,  15,  92,  85, 183, 168, 111,\n",
       "          0,  76,  80, 133,  13,  36, 136,  95, 135,  33, 174,  90, 108,\n",
       "        162, 148, 118,  11, 119,  98,  19, 190, 155,  32, 112,  38,  56,\n",
       "        157, 160, 138, 173,   8, 149,  23,  96,  63,  42, 106, 188, 114,\n",
       "        109, 163, 170, 140,  29,  22, 189, 153, 156,  35, 141, 171,  49,\n",
       "         89,  39, 105, 122, 129,  30, 147, 104,  40, 178,  47, 180, 127,\n",
       "          3,  40,  79,  17,  27,  59,  45, 154,  48, 101, 115,  18, 116,\n",
       "        132,   2,  60, 120,  10,  34, 169,  41, 164,  74, 128,  55, 151,\n",
       "         51, 137,  71, 173,  49,  67, 138, 121, 110, 126, 161, 158,  84,\n",
       "         46, 131,   4, 181,  25], dtype=int64),\n",
       " 'gender': array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "        1, 1]),\n",
       " 'age': array([2, 1, 2, 1, 5, 1, 0, 5, 2, 5, 3, 2, 1, 6, 2, 1, 2, 3, 3, 1, 2, 4,\n",
       "        4, 3, 2, 6, 2, 6, 4, 2, 1, 3, 2, 1, 5, 4, 1, 4, 3, 3, 5, 2, 5, 2,\n",
       "        5, 5, 0, 2, 6, 2, 3, 1, 2, 3, 3, 2, 1, 1, 4, 5, 2, 2, 1, 1, 5, 3,\n",
       "        3, 3, 2, 6, 2, 4, 2, 3, 2, 2, 6, 4, 5, 2, 4, 2, 2, 2, 1, 5, 5, 3,\n",
       "        1, 3, 3, 3, 5, 2, 2, 2, 1, 2, 5, 2, 1, 2, 3, 1, 3, 2, 1, 2, 2, 3,\n",
       "        1, 1, 6, 1, 3, 3, 2, 1, 5, 3, 1, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 4,\n",
       "        2, 2, 2, 0, 0, 2, 5, 2, 5, 2, 2, 2, 1, 5, 6, 5, 3, 5, 2, 3, 2, 2,\n",
       "        3, 0, 1, 3, 5, 6, 5, 2, 2, 6, 2, 2, 1, 2, 4, 2, 1, 1, 4, 3, 2, 1,\n",
       "        2, 5, 1, 3, 3, 2, 5, 2, 6, 1, 2, 2, 3, 1, 3, 4, 6, 3, 0, 2, 1, 1,\n",
       "        2, 2], dtype=int64),\n",
       " 'occupation': array([ 4,  4, 13, 18, 19,  0,  1,  1, 16, 10, 19,  2,  4,  1,  4,  0, 14,\n",
       "         7, 16,  0,  1, 13,  7, 14,  2,  7, 19, 12,  1, 11,  0,  0,  4, 19,\n",
       "         1,  1,  4,  1,  3, 19, 14,  8,  0, 16, 15, 15,  9,  0, 15, 11,  5,\n",
       "         0,  0,  0,  5, 11, 16, 13,  6, 13,  0,  0,  4,  4,  6, 14, 16,  2,\n",
       "         2,  0,  0,  8,  7, 18,  7, 10,  1, 19,  7,  7,  1,  3,  6,  0, 13,\n",
       "        16, 19,  1, 14,  1, 19,  7, 15,  0,  0,  0,  4,  0, 11, 14,  4, 10,\n",
       "        10,  4,  4,  1, 16, 15,  0, 11,  4,  5,  1,  4,  1,  2, 19,  4, 13,\n",
       "        15, 11,  0,  4, 15,  0, 18, 13, 19,  7,  0, 11,  1, 15, 16,  0, 18,\n",
       "         9, 13, 17, 18, 11, 11,  0, 13,  1, 12, 16,  6,  3,  6, 17,  4,  6,\n",
       "         7, 16,  9,  1,  4, 11, 16,  0,  3,  0,  1, 11,  3,  4,  0,  7, 19,\n",
       "         4,  4,  0,  7,  3,  9, 18, 11,  4,  6, 11,  2,  7, 18, 12, 11,  0,\n",
       "         7, 15, 14,  7,  7,  2, 16,  9, 11, 11, 13,  0,  0], dtype=int64),\n",
       " 'zip': array([ 35, 118,  99,  55,  41, 108, 137,  45,  84, 144, 178, 140,  31,\n",
       "         86, 112,  94,  62,  16,  39, 108, 142, 165,   3,  44, 135,  70,\n",
       "        166,   9,  33,  96, 162, 155,  79,  85,  69, 145, 149, 136, 187,\n",
       "         23,  95, 150,   1,   2, 143, 184,  72,  19,  61, 123, 172,  88,\n",
       "         78,  10,  25, 139,  90, 117,   4,  32,  21, 164,   0, 152,  60,\n",
       "         44, 157, 107, 115, 119, 146, 169, 102, 159, 103,   6,  76, 148,\n",
       "         71,  40, 105,  87,  26,  73,  20,  64,  54,  50, 153, 114,  59,\n",
       "        100,  15,  77, 127,  68, 181,  42, 177, 126, 168,  99,  80, 130,\n",
       "        174, 141, 111,  93, 183, 171, 180, 175,  71, 132,  43,  36,  37,\n",
       "        109,  98,  75, 134,  97, 186,  74,  22,  29,  66,   5,  11, 183,\n",
       "        120,  34,  58, 185,  21,  89,  46,  52,  14, 128,  63, 161, 104,\n",
       "         38, 158, 154,  49,  30,  27,  81,  47, 179,   8, 133,  17,  12,\n",
       "         24, 179, 125,  65, 182, 173, 131,  51, 147,  92, 122,  67,  57,\n",
       "        138,  13, 118, 121,  18, 124, 110, 156, 167, 101,  91, 170, 176,\n",
       "         53,  28, 163, 134, 104, 151,  75,   7, 116,  56, 160, 129,  82,\n",
       "         48, 113,  83, 106, 136]),\n",
       " 'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c527904-2bd0-48df-a3ec-e0fcfad0b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 14.2872 - mse: 14.2872 - val_loss: 13.4153 - val_mse: 13.4153\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.1986 - mse: 14.1986 - val_loss: 13.3399 - val_mse: 13.3399\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.1121 - mse: 14.1121 - val_loss: 13.2637 - val_mse: 13.2637\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 14.0272 - mse: 14.0272 - val_loss: 13.1860 - val_mse: 13.1860\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 13.9424 - mse: 13.9424 - val_loss: 13.1077 - val_mse: 13.1077\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.8566 - mse: 13.8566 - val_loss: 13.0273 - val_mse: 13.0273\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.7692 - mse: 13.7692 - val_loss: 12.9449 - val_mse: 12.9449\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.6794 - mse: 13.6794 - val_loss: 12.8597 - val_mse: 12.8597\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.5866 - mse: 13.5866 - val_loss: 12.7710 - val_mse: 12.7710\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.4901 - mse: 13.4901 - val_loss: 12.6785 - val_mse: 12.6785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d807f2b0d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}