{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa67662-0019-4f4c-ae07-4a5c444c9a8b",
   "metadata": {},
   "source": [
    "###  变长序列的处理方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124c136-7560-4a0d-bade-d3e65e5a1f13",
   "metadata": {},
   "source": [
    "变长序列的处理逻辑就是：\n",
    "\n",
    "👉 虽然原始的变长序列（如 genres、keywords）在输入时是一个序列，但在经过 embedding 和 pooling（这里是 mean pooling）之后，会被压缩成一个定长向量，相当于一个“整体语义”表示。\n",
    "换句话说：\n",
    "\n",
    "原始序列（如 ['动作', '冒险', '科幻']）\n",
    "⬇️\n",
    "通过 Embedding → 变成 (batch_size, seq_len, embedding_dim) 的张量\n",
    "⬇️\n",
    "通过 mean pooling → 汇总成 (batch_size, 1, embedding_dim)\n",
    "⬇️\n",
    "和其它稀疏特征一起参与拼接、建模\n",
    "所以最终：\n",
    "\n",
    "无论是稀疏特征（单个索引）还是变长特征（序列），最终都统一成 (batch_size, 1, embedding_dim) 的形式，这样才能拼接起来喂给 FM 和 DNN。\n",
    "\n",
    "这种做法其实就是“把变长特征转化为定长特征”的通用套路之一（还有比如 attention pooling、CNN、RNN 都可以做类似事情）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34f13-7086-4c45-91bd-2e114dc9aa10",
   "metadata": {},
   "source": [
    "###  整体程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d75a1f-a740-4b19-ab6e-e8a3314bb925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 14.2088 - mse: 14.2088 - val_loss: 13.3243 - val_mse: 13.3243\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 14.1188 - mse: 14.1188 - val_loss: 13.2393 - val_mse: 13.2393\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 14.0249 - mse: 14.0249 - val_loss: 13.1507 - val_mse: 13.1507\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.9267 - mse: 13.9267 - val_loss: 13.0584 - val_mse: 13.0584\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.8235 - mse: 13.8235 - val_loss: 12.9604 - val_mse: 12.9604\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.7151 - mse: 13.7151 - val_loss: 12.8573 - val_mse: 12.8573\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.6011 - mse: 13.6011 - val_loss: 12.7488 - val_mse: 12.7488\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.4810 - mse: 13.4810 - val_loss: 12.6347 - val_mse: 12.6347\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.3550 - mse: 13.3550 - val_loss: 12.5153 - val_mse: 12.5153\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.2227 - mse: 13.2227 - val_loss: 12.3897 - val_mse: 12.3897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x183060d6970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()\n",
    "\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]\n",
    "\n",
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}\n",
    "\n",
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）\n",
    "\n",
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num\n",
    "\n",
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)\n",
    "\n",
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "\n",
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) 从这可以看出边长序列的字段数据最终整体也是当成一个字段处理\n",
    "    embeds.append(pooled_embed)\n",
    "\n",
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "\n",
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)\n",
    "\n",
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)\n",
    "\n",
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "# 7. 编译并训练模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# 构建模型输入字典\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_len)\n",
    "\n",
    "# 模型训练\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4c309-8eb3-489e-b655-94ecad0bfbd8",
   "metadata": {},
   "source": [
    "### 详细注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39765f5-5f8b-437d-8dd3-9b50f9b92bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>genres_bak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3299</td>\n",
       "      <td>235</td>\n",
       "      <td>4</td>\n",
       "      <td>968035345</td>\n",
       "      <td>Ed Wood (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>19119</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3630</td>\n",
       "      <td>3256</td>\n",
       "      <td>3</td>\n",
       "      <td>966536874</td>\n",
       "      <td>Patriot Games (1992)</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>77005</td>\n",
       "      <td>Action|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>976203603</td>\n",
       "      <td>Bridges of Madison County, The (1995)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>55408</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>785</td>\n",
       "      <td>2115</td>\n",
       "      <td>3</td>\n",
       "      <td>975430389</td>\n",
       "      <td>Indiana Jones and the Temple of Doom (1984)</td>\n",
       "      <td>Action|Adventure</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>29307</td>\n",
       "      <td>Action|Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5848</td>\n",
       "      <td>909</td>\n",
       "      <td>5</td>\n",
       "      <td>957782527</td>\n",
       "      <td>Apartment, The (1960)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>20009</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0     3299       235       4  968035345   \n",
       "1     3630      3256       3  966536874   \n",
       "2      517       105       4  976203603   \n",
       "3      785      2115       3  975430389   \n",
       "4     5848       909       5  957782527   \n",
       "\n",
       "                                         title            genres gender  age  \\\n",
       "0                               Ed Wood (1994)      Comedy|Drama      F   25   \n",
       "1                         Patriot Games (1992)   Action|Thriller      M   18   \n",
       "2        Bridges of Madison County, The (1995)     Drama|Romance      F   25   \n",
       "3  Indiana Jones and the Temple of Doom (1984)  Action|Adventure      M   18   \n",
       "4                        Apartment, The (1960)      Comedy|Drama      M   50   \n",
       "\n",
       "   occupation    zip        genres_bak  \n",
       "0           4  19119      Comedy|Drama  \n",
       "1           4  77005   Action|Thriller  \n",
       "2          14  55408     Drama|Romance  \n",
       "3          19  29307  Action|Adventure  \n",
       "4          20  20009      Comedy|Drama  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 加载并预处理数据，这里模拟出2个变长序列数据\n",
    "data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "data['genres_bak'] = data['genres']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50a39bb-8a3f-4f91-a801-745f3705c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = [\"rating\"]\n",
    "sequence_features = [\"genres\", \"genres_bak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae6578a-034a-4dc5-a905-aececfa9c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对稀疏特征做标签编码（Label Encoding）\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e804ee24-a602-47ac-b296-448d946b3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于存储每个变长特征处理后的 padding 序列\n",
    "pad_sequences_dict = {}\n",
    "\n",
    "# 每个变长特征对应一个独立的 Tokenizer，用于后续文本转索引\n",
    "tokenizers = {}\n",
    "\n",
    "# 用于记录每个变长特征的 padding 长度（即序列被填充后的最大长度）\n",
    "pad_len_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9329ef-f738-40ae-9d8d-dd9b477d92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历所有变长序列特征\n",
    "for feature in sequence_features:\n",
    "    texts = data[feature].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "    tokenizer = Tokenizer(oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, padding='post')  # shape: (num_samples, max_seq_len)\n",
    "    pad_sequences_dict[feature] = padded\n",
    "    tokenizers[feature] = tokenizer\n",
    "    pad_len_dict[feature] = padded.shape[1]  # 保存每个特征的序列长度（max_seq_len）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92368043-47cc-4c30-8af6-c7e4db4f76a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a745c17-2d69-4ea0-8953-cffde01d7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': <keras_preprocessing.text.Tokenizer at 0x1830946b730>,\n",
       " 'genres_bak': <keras_preprocessing.text.Tokenizer at 0x182ed0018e0>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94580812-7dfa-49f7-a126-398e8472fe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genres': 6, 'genres_bak': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e57c9e-5da6-48d6-8937-17de54c3c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 创建所有特征的Embedding层\n",
    "embedding_dim = 4\n",
    "vocab_sizes = {feat: data[feat].nunique() for feat in sparse_features}\n",
    "\n",
    "for feature in sequence_features:\n",
    "    feat_num = len(tokenizers[feature].word_index) + 1\n",
    "    vocab_sizes[feature] = feat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe7bd947-80bb-4bab-928a-f737e887004a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': 187,\n",
       " 'user_id': 193,\n",
       " 'gender': 2,\n",
       " 'age': 7,\n",
       " 'occupation': 20,\n",
       " 'zip': 188,\n",
       " 'genres': 21,\n",
       " 'genres_bak': 21}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1750201-5824-44d7-a523-d67c02ca7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建嵌入层字典\n",
    "embed_layers = {}\n",
    "for feat in sparse_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=False)\n",
    "for feat in sequence_features:\n",
    "    embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1, output_dim=embedding_dim, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3143e25b-acc5-47a6-9d19-d4fab11b1d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <keras.layers.embeddings.Embedding at 0x1830946f7c0>,\n",
       " 'user_id': <keras.layers.embeddings.Embedding at 0x1830946f910>,\n",
       " 'gender': <keras.layers.embeddings.Embedding at 0x1830946fb80>,\n",
       " 'age': <keras.layers.embeddings.Embedding at 0x1830946feb0>,\n",
       " 'occupation': <keras.layers.embeddings.Embedding at 0x1830946f9a0>,\n",
       " 'zip': <keras.layers.embeddings.Embedding at 0x1830946bcd0>,\n",
       " 'genres': <keras.layers.embeddings.Embedding at 0x1830946f250>,\n",
       " 'genres_bak': <keras.layers.embeddings.Embedding at 0x1830946bc10>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e9383e-7c44-4970-a85d-6562810e9b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 初始化模型输入字典\n",
    "inputs = {}\n",
    "for feat in sparse_features:\n",
    "    inputs[feat] = tf.keras.Input(shape=(1,), name=feat, dtype=tf.int32)  # shape: (batch_size, 1)\n",
    "for feat in sequence_features:\n",
    "    max_len = pad_len_dict[feat]\n",
    "    inputs[feat] = tf.keras.Input(shape=(max_len,), name=feat, dtype=tf.int32)  # shape: (batch_size, max_len)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eedf480-5f59-4b65-82c7-9d9563799a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建嵌入列表\n",
    "embeds = []\n",
    "for feat in sparse_features:\n",
    "    embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, 1, embedding_dim)\n",
    "    embeds.append(embed)\n",
    "\n",
    "for feat in sequence_features:\n",
    "    seq_embed = embed_layers[feat](inputs[feat])  # shape: (batch_size, seq_len, embedding_dim)\n",
    "    pooled_embed = tf.reduce_mean(seq_embed, axis=1, keepdims=True)  # shape: (batch_size, 1, embedding_dim) 从这可以看出边长序列的字段数据最终整体也是当成一个字段处理\n",
    "    embeds.append(pooled_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd503f56-a6b5-44dc-843f-bea013606eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_8')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_9')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_10')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_11')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_12')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'embedding_13')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_2')>,\n",
       " <KerasTensor: shape=(None, 1, 4) dtype=float32 (created by layer 'tf.math.reduce_mean_3')>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "487d73ea-32a2-4df1-a2ff-1d64fc8eba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 4) dtype=float32 (created by layer 'tf.concat_1')>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拼接所有嵌入特征\n",
    "total_embeds = tf.concat(embeds, axis=1)  # shape: (batch_size, num_fields, embedding_dim)\n",
    "total_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef2679a8-b53b-477e-9f7d-860a8324dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FM 二阶交叉项计算\n",
    "sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))  # shape: (batch_size, embedding_dim)\n",
    "square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)  # shape: (batch_size, embedding_dim)\n",
    "fm_second_order = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)  # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13fe9dd5-2113-4ead-bbd3-3f5fb657a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DNN 部分\n",
    "flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * embedding_dim))  # shape: (batch_size, num_fields * embedding_dim)\n",
    "x = layers.Dense(64, activation='relu')(flatten_input)  # shape: (batch_size, 64)\n",
    "x = layers.Dense(32, activation='relu')(x)              # shape: (batch_size, 32)\n",
    "dnn_output = layers.Dense(1)(x)                         # shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29622049-2cbb-42c3-80c3-5cee47fe99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'add_1')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 合并 FM 和 DNN 输出结果\n",
    "output = layers.Add()([fm_second_order, dnn_output])    # shape: (batch_size, 1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbcdf14a-64cd-4c7d-9718-0eeae6fb1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b665b19-2dae-4260-b92b-d458d496f70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x183094ebe20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e303ef3-a2af-437d-b54c-ebc290d5cc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " 'user_id': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " 'gender': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " 'occupation': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " 'zip': <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " 'genres': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " 'genres_bak': <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "811b4a3c-b842-4100-aa02-0b287bc1a40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>, <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>, <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e049473-510d-4c86-bcae-22b9c1ffbe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'movie_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'user_id')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'gender')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'age')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'occupation')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=int32 (created by layer 'zip')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres')>,\n",
       " <KerasTensor: shape=(None, 6) dtype=int32 (created by layer 'genres_bak')>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=list(inputs.values())\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b64757c7-ed96-4267-8449-231844931675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 编译并训练模型\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# 构建模型输入字典\n",
    "model_input = {}\n",
    "for feat in sparse_features:\n",
    "    model_input[feat] = data[feat].values  # shape: (num_samples,)\n",
    "for feat in sequence_features:\n",
    "    model_input[feat] = pad_sequences_dict[feat]  # shape: (num_samples, max_seq_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12f55d95-f94e-4f84-9c89-836a8ee3b9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': array([ 12, 169,   6, 112,  45, 146,  43, 156,  30, 174,  82, 173,  91,\n",
       "        108, 132,  40, 109,  31, 180, 183, 129,  67, 137,  87, 127,   8,\n",
       "        104, 100, 140,  25, 122, 124, 116, 126,  72, 117,  42, 145, 131,\n",
       "          2,  52,  17, 101,  94, 136,  65,  20, 144,  26,  83,  55, 126,\n",
       "        184,  23, 121, 142,  33,   0,  46, 139, 150, 135,  36, 110,  79,\n",
       "        162,  70, 147,   9,  34,   7,  76,   4, 185,  73, 112, 130,  95,\n",
       "         28,  24, 148, 119, 168, 149, 181,  13, 154,  56,  66, 172,  69,\n",
       "         35,  49, 106,  35,  11, 152, 166,  37, 164,  54, 167,  72,  29,\n",
       "         92, 114,  88, 170,  64,  60,  38,  22,  62, 178, 134, 157,  99,\n",
       "         34, 111,  96,  50,  75,  47,  14,  21,  77, 118, 182, 113, 143,\n",
       "        149, 141,  10,  58,  81,  44,  27, 151, 165,  98, 163,  80, 158,\n",
       "        161,  27, 155, 171,  78,  57, 123,  84,  93, 170, 120,   1, 153,\n",
       "         39,  61,  51,  71,  19, 107,   9,  66, 102,  74, 177, 103, 133,\n",
       "        160,  53,  90,   5, 173,  41,  59, 123, 159,  48, 115, 138,  63,\n",
       "         16, 179,   3,  97, 128, 186, 175, 105, 169,  32,  68,  18,  85,\n",
       "        176,  89, 125,  15,  86], dtype=int64),\n",
       " 'user_id': array([107, 123,  12,  21, 187,  99, 102,  24, 134,  68,  97,   7,  77,\n",
       "          1, 125,  78, 191, 124, 145,  99, 186,  69, 175,  44, 103, 150,\n",
       "         54,  73, 113,   9,  65, 159, 144,  37,  86, 176,  16, 100,  75,\n",
       "         14,   6, 165, 143, 177,  66,  53,  20,  64, 179,  52, 152,  94,\n",
       "         61,  93,  62,   5,  91,  26,  81,  83,  29,  50,  58,  88,  57,\n",
       "         44,  72, 117, 184, 172, 185,  31, 130,  28, 166, 182, 146,  70,\n",
       "        139,  43,  82,  87, 192, 142, 167,  15,  92,  85, 183, 168, 111,\n",
       "          0,  76,  80, 133,  13,  36, 136,  95, 135,  33, 174,  90, 108,\n",
       "        162, 148, 118,  11, 119,  98,  19, 190, 155,  32, 112,  38,  56,\n",
       "        157, 160, 138, 173,   8, 149,  23,  96,  63,  42, 106, 188, 114,\n",
       "        109, 163, 170, 140,  29,  22, 189, 153, 156,  35, 141, 171,  49,\n",
       "         89,  39, 105, 122, 129,  30, 147, 104,  40, 178,  47, 180, 127,\n",
       "          3,  40,  79,  17,  27,  59,  45, 154,  48, 101, 115,  18, 116,\n",
       "        132,   2,  60, 120,  10,  34, 169,  41, 164,  74, 128,  55, 151,\n",
       "         51, 137,  71, 173,  49,  67, 138, 121, 110, 126, 161, 158,  84,\n",
       "         46, 131,   4, 181,  25], dtype=int64),\n",
       " 'gender': array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "        1, 1]),\n",
       " 'age': array([2, 1, 2, 1, 5, 1, 0, 5, 2, 5, 3, 2, 1, 6, 2, 1, 2, 3, 3, 1, 2, 4,\n",
       "        4, 3, 2, 6, 2, 6, 4, 2, 1, 3, 2, 1, 5, 4, 1, 4, 3, 3, 5, 2, 5, 2,\n",
       "        5, 5, 0, 2, 6, 2, 3, 1, 2, 3, 3, 2, 1, 1, 4, 5, 2, 2, 1, 1, 5, 3,\n",
       "        3, 3, 2, 6, 2, 4, 2, 3, 2, 2, 6, 4, 5, 2, 4, 2, 2, 2, 1, 5, 5, 3,\n",
       "        1, 3, 3, 3, 5, 2, 2, 2, 1, 2, 5, 2, 1, 2, 3, 1, 3, 2, 1, 2, 2, 3,\n",
       "        1, 1, 6, 1, 3, 3, 2, 1, 5, 3, 1, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 4,\n",
       "        2, 2, 2, 0, 0, 2, 5, 2, 5, 2, 2, 2, 1, 5, 6, 5, 3, 5, 2, 3, 2, 2,\n",
       "        3, 0, 1, 3, 5, 6, 5, 2, 2, 6, 2, 2, 1, 2, 4, 2, 1, 1, 4, 3, 2, 1,\n",
       "        2, 5, 1, 3, 3, 2, 5, 2, 6, 1, 2, 2, 3, 1, 3, 4, 6, 3, 0, 2, 1, 1,\n",
       "        2, 2], dtype=int64),\n",
       " 'occupation': array([ 4,  4, 13, 18, 19,  0,  1,  1, 16, 10, 19,  2,  4,  1,  4,  0, 14,\n",
       "         7, 16,  0,  1, 13,  7, 14,  2,  7, 19, 12,  1, 11,  0,  0,  4, 19,\n",
       "         1,  1,  4,  1,  3, 19, 14,  8,  0, 16, 15, 15,  9,  0, 15, 11,  5,\n",
       "         0,  0,  0,  5, 11, 16, 13,  6, 13,  0,  0,  4,  4,  6, 14, 16,  2,\n",
       "         2,  0,  0,  8,  7, 18,  7, 10,  1, 19,  7,  7,  1,  3,  6,  0, 13,\n",
       "        16, 19,  1, 14,  1, 19,  7, 15,  0,  0,  0,  4,  0, 11, 14,  4, 10,\n",
       "        10,  4,  4,  1, 16, 15,  0, 11,  4,  5,  1,  4,  1,  2, 19,  4, 13,\n",
       "        15, 11,  0,  4, 15,  0, 18, 13, 19,  7,  0, 11,  1, 15, 16,  0, 18,\n",
       "         9, 13, 17, 18, 11, 11,  0, 13,  1, 12, 16,  6,  3,  6, 17,  4,  6,\n",
       "         7, 16,  9,  1,  4, 11, 16,  0,  3,  0,  1, 11,  3,  4,  0,  7, 19,\n",
       "         4,  4,  0,  7,  3,  9, 18, 11,  4,  6, 11,  2,  7, 18, 12, 11,  0,\n",
       "         7, 15, 14,  7,  7,  2, 16,  9, 11, 11, 13,  0,  0], dtype=int64),\n",
       " 'zip': array([ 35, 118,  99,  55,  41, 108, 137,  45,  84, 144, 178, 140,  31,\n",
       "         86, 112,  94,  62,  16,  39, 108, 142, 165,   3,  44, 135,  70,\n",
       "        166,   9,  33,  96, 162, 155,  79,  85,  69, 145, 149, 136, 187,\n",
       "         23,  95, 150,   1,   2, 143, 184,  72,  19,  61, 123, 172,  88,\n",
       "         78,  10,  25, 139,  90, 117,   4,  32,  21, 164,   0, 152,  60,\n",
       "         44, 157, 107, 115, 119, 146, 169, 102, 159, 103,   6,  76, 148,\n",
       "         71,  40, 105,  87,  26,  73,  20,  64,  54,  50, 153, 114,  59,\n",
       "        100,  15,  77, 127,  68, 181,  42, 177, 126, 168,  99,  80, 130,\n",
       "        174, 141, 111,  93, 183, 171, 180, 175,  71, 132,  43,  36,  37,\n",
       "        109,  98,  75, 134,  97, 186,  74,  22,  29,  66,   5,  11, 183,\n",
       "        120,  34,  58, 185,  21,  89,  46,  52,  14, 128,  63, 161, 104,\n",
       "         38, 158, 154,  49,  30,  27,  81,  47, 179,   8, 133,  17,  12,\n",
       "         24, 179, 125,  65, 182, 173, 131,  51, 147,  92, 122,  67,  57,\n",
       "        138,  13, 118, 121,  18, 124, 110, 156, 167, 101,  91, 170, 176,\n",
       "         53,  28, 163, 134, 104, 151,  75,   7, 116,  56, 160, 129,  82,\n",
       "         48, 113,  83, 106, 136]),\n",
       " 'genres': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]]),\n",
       " 'genres_bak': array([[2, 3, 0, 0, 0, 0],\n",
       "        [4, 5, 0, 0, 0, 0],\n",
       "        [3, 6, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 6, 0, 0, 0, 0],\n",
       "        [4, 9, 5, 0, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c527904-2bd0-48df-a3ec-e0fcfad0b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 14.3072 - mse: 14.3072 - val_loss: 13.4602 - val_mse: 13.4602\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14.2007 - mse: 14.2007 - val_loss: 13.3670 - val_mse: 13.3670\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14.0969 - mse: 14.0969 - val_loss: 13.2767 - val_mse: 13.2767\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.9932 - mse: 13.9932 - val_loss: 13.1862 - val_mse: 13.1862\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.8870 - mse: 13.8870 - val_loss: 13.0909 - val_mse: 13.0909\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.7778 - mse: 13.7778 - val_loss: 12.9928 - val_mse: 12.9928\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.6647 - mse: 13.6647 - val_loss: 12.8905 - val_mse: 12.8905\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.5466 - mse: 13.5466 - val_loss: 12.7832 - val_mse: 12.7832\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.4233 - mse: 13.4233 - val_loss: 12.6712 - val_mse: 12.6712\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.2935 - mse: 13.2935 - val_loss: 12.5533 - val_mse: 12.5533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1830952cbe0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0795f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c0c8ea2",
   "metadata": {},
   "source": [
    "### 模型封装为类的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de26bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "[<tf.Tensor 'fm_dnn__model/embedding/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_1/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_2/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_3/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_4/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_5/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/Mean:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/Mean_1:0' shape=(None, 1, 4) dtype=float32>]\n",
      "[<tf.Tensor 'fm_dnn__model/embedding/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_1/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_2/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_3/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_4/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_5/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/Mean:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/Mean_1:0' shape=(None, 1, 4) dtype=float32>]\n",
      "1/1 [==============================] - ETA: 0s - loss: 14.3404 - mse: 14.3404[<tf.Tensor 'fm_dnn__model/embedding/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_1/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_2/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_3/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_4/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/embedding_5/embedding_lookup/Identity_1:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/Mean:0' shape=(None, 1, 4) dtype=float32>, <tf.Tensor 'fm_dnn__model/Mean_1:0' shape=(None, 1, 4) dtype=float32>]\n",
      "1/1 [==============================] - 2s 2s/step - loss: 14.3404 - mse: 14.3404 - val_loss: 13.4843 - val_mse: 13.4843\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.2594 - mse: 14.2594 - val_loss: 13.4137 - val_mse: 13.4137\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14.1798 - mse: 14.1798 - val_loss: 13.3438 - val_mse: 13.3438\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 14.1008 - mse: 14.1008 - val_loss: 13.2735 - val_mse: 13.2735\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.0208 - mse: 14.0208 - val_loss: 13.2012 - val_mse: 13.2012\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 13.9389 - mse: 13.9389 - val_loss: 13.1285 - val_mse: 13.1285\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.8548 - mse: 13.8548 - val_loss: 13.0533 - val_mse: 13.0533\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.7678 - mse: 13.7678 - val_loss: 12.9755 - val_mse: 12.9755\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.6779 - mse: 13.6779 - val_loss: 12.8946 - val_mse: 12.8946\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.5844 - mse: 13.5844 - val_loss: 12.8103 - val_mse: 12.8103\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class FeatureProcessor:\n",
    "    def __init__(self, sparse_features, sequence_features, embedding_dim=4):\n",
    "        self.sparse_features = sparse_features\n",
    "        self.sequence_features = sequence_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.label_encoders = {}\n",
    "        self.tokenizers = {}\n",
    "        self.pad_sequences_dict = {}\n",
    "        self.pad_len_dict = {}\n",
    "        self.vocab_sizes = {}\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        # 编码稀疏特征\n",
    "        for feat in self.sparse_features:\n",
    "            lbe = LabelEncoder()\n",
    "            df[feat] = lbe.fit_transform(df[feat])\n",
    "            self.label_encoders[feat] = lbe\n",
    "            self.vocab_sizes[feat] = df[feat].nunique()\n",
    "\n",
    "        # 编码变长序列特征\n",
    "        for feat in self.sequence_features:\n",
    "            texts = df[feat].apply(lambda x: x.replace('|', ' ')).tolist()\n",
    "            tokenizer = Tokenizer(oov_token='OOV')\n",
    "            tokenizer.fit_on_texts(texts)\n",
    "            sequences = tokenizer.texts_to_sequences(texts)\n",
    "            padded = pad_sequences(sequences, padding='post')\n",
    "            self.tokenizers[feat] = tokenizer\n",
    "            self.pad_sequences_dict[feat] = padded\n",
    "            self.pad_len_dict[feat] = padded.shape[1]\n",
    "            self.vocab_sizes[feat] = len(tokenizer.word_index) + 1\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_model_input(self, df):\n",
    "        model_input = {}\n",
    "        for feat in self.sparse_features:\n",
    "            model_input[feat] = df[feat].values\n",
    "        for feat in self.sequence_features:\n",
    "            model_input[feat] = self.pad_sequences_dict[feat]\n",
    "        return model_input\n",
    "\n",
    "\n",
    "class FM_DNN_Model(tf.keras.Model):\n",
    "    def __init__(self, sparse_features, sequence_features, vocab_sizes, pad_len_dict, embedding_dim=4):\n",
    "        super(FM_DNN_Model, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sparse_features = sparse_features\n",
    "        self.sequence_features = sequence_features\n",
    "        self.vocab_sizes = vocab_sizes\n",
    "        self.pad_len_dict = pad_len_dict\n",
    "\n",
    "        self.embed_layers = {}\n",
    "        for feat in sparse_features:\n",
    "            self.embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1,\n",
    "                                                       output_dim=embedding_dim,\n",
    "                                                       mask_zero=False)\n",
    "\n",
    "        for feat in sequence_features:\n",
    "            self.embed_layers[feat] = layers.Embedding(input_dim=vocab_sizes[feat] + 1,\n",
    "                                                       output_dim=embedding_dim,\n",
    "                                                       mask_zero=True)\n",
    "\n",
    "        # DNN 部分\n",
    "        self.dnn = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeds = []\n",
    "\n",
    "        # 稀疏特征嵌入 shape: (batch, 1, embed_dim)\n",
    "        for feat in self.sparse_features:\n",
    "            embed = self.embed_layers[feat](inputs[feat])\n",
    "            embeds.append(embed)\n",
    "\n",
    "        # 序列特征嵌入并池化 shape: (batch, 1, embed_dim)\n",
    "        for feat in self.sequence_features:\n",
    "            embed = self.embed_layers[feat](inputs[feat])\n",
    "            pooled = tf.reduce_mean(embed, axis=1, keepdims=True)\n",
    "            embeds.append(pooled)\n",
    "\n",
    "        print(embeds)\n",
    "        \n",
    "        total_embeds = tf.concat(embeds, axis=1)  # shape: (batch, field_num, embed_dim)\n",
    "\n",
    "        # FM 二阶交叉项\n",
    "        sum_square = tf.square(tf.reduce_sum(total_embeds, axis=1))\n",
    "        square_sum = tf.reduce_sum(tf.square(total_embeds), axis=1)\n",
    "        fm_output = 0.5 * tf.reduce_sum(sum_square - square_sum, axis=1, keepdims=True)\n",
    "\n",
    "        # DNN 输入\n",
    "        flatten_input = tf.reshape(total_embeds, shape=(-1, total_embeds.shape[1] * self.embedding_dim))\n",
    "        dnn_output = self.dnn(flatten_input)\n",
    "\n",
    "        # FM + DNN\n",
    "        return fm_output + dnn_output\n",
    "\n",
    "\n",
    "# =================== 示例使用 ===================\n",
    "if __name__ == '__main__':\n",
    "    # 读取数据\n",
    "    data = pd.read_csv(r\"D:\\software\\pycharm_repository\\StarMaker\\MultiRecSys\\data_files\\movielens_sample.txt\")\n",
    "    data['genres_bak'] = data['genres']\n",
    "\n",
    "    sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "    sequence_features = [\"genres\", \"genres_bak\"]\n",
    "    target = [\"rating\"]\n",
    "\n",
    "    # 特征处理\n",
    "    processor = FeatureProcessor(sparse_features, sequence_features, embedding_dim=4)\n",
    "    data = processor.fit_transform(data)\n",
    "    model_input = processor.get_model_input(data)\n",
    "\n",
    "    # 模型构建\n",
    "    model = FM_DNN_Model(sparse_features, sequence_features,\n",
    "                         vocab_sizes=processor.vocab_sizes,\n",
    "                         pad_len_dict=processor.pad_len_dict,\n",
    "                         embedding_dim=4)\n",
    "\n",
    "    # 模型编译 & 训练\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    model.fit(model_input, data[target].values, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208a765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335495a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
