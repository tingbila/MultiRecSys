# !/usr/bin/env python
# -*- coding:utf-8 -*-
# @Time  : 2018.
# @Author : å¼ æ˜é˜³
# @Email : mingyang.zhang@ushow.media

# âœ… æ•´ä½“ç»“æ„ï¼š
# âœ… ä½¿ç”¨äº† å¤šä¸ª expert ç½‘ç»œ æ¥æå–å…±äº«ç‰¹å¾ã€‚
# âœ… å¯¹æ¯ä¸ªä»»åŠ¡éƒ½ä½¿ç”¨ç‹¬ç«‹çš„ gate ç½‘ç»œ è¾“å‡º softmax æƒé‡ã€‚
# âœ… ç”¨ gate æƒé‡å¯¹ expert è¾“å‡ºåŠ æƒæ±‚å’Œï¼Œä½œä¸ºæ¯ä¸ªä»»åŠ¡çš„è¾“å…¥ã€‚
# âœ… æ”¯æŒ ç¨ å¯†ç‰¹å¾ + ç¦»æ•£ç‰¹å¾ + å˜é•¿åºåˆ—ç‰¹å¾ çš„èåˆå»ºæ¨¡ã€‚
# âœ… æ¯ä¸ªä»»åŠ¡éƒ½ä½¿ç”¨ç‹¬ç«‹çš„è¾“å‡ºå±‚ Dense(1, activation='sigmoid')ã€‚

from tensorflow.keras import layers, Model
import tensorflow as tf
import numpy as np
import tensorflow as tf
import numpy as np
from config.data_config import *
from models.Attention import Attention

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

class MMOE_NLP_Attention(Model):
    def __init__(self, feat_columns, emb_size=5, num_experts=4):
        super().__init__()
        self.dense_feats, self.sparse_feats = feat_columns[0], feat_columns[1]
        self.seq_feats = feat_columns[2] if len(feat_columns) > 2 else []
        
        self.emb_size = emb_size
        self.num_experts = num_experts

        # Embedding for sparse features
        self.sparse_embs = [layers.Embedding(input_dim=feat['feat_num'], output_dim=self.emb_size) for feat in self.sparse_feats]

        # Sequence embedding layers
        # ä»…å½“å­˜åœ¨åºåˆ—ç‰¹å¾æ—¶æ‰æ„é€  seq embedding å±‚
        if self.seq_feats:
            self.seq_embeds = [layers.Embedding(input_dim=feat['feat_num'], output_dim=self.emb_size, mask_zero=True) for feat in self.seq_feats]

        # Expert networks
        self.experts = [
            tf.keras.Sequential([
                layers.Dense(128, activation='relu'),
                layers.Dropout(0.2),
                layers.Dense(64, activation='relu')
            ]) for _ in range(self.num_experts)
        ]

        # Gate networks for each task   ====> ç”¨æ¥è®¡ç®—æ¯ä¸ªä¸“å®¶å¯¹åº”çš„gateçš„æƒé‡
        self.gate_finish = layers.Dense(self.num_experts, activation='softmax')
        self.gate_like   = layers.Dense(self.num_experts, activation='softmax')

        # Output layers for each task  ====> ç”¨æ¥è®¡ç®—æ¯ä¸ªå¡”çš„è¾“å‡ºï¼Œå¯¹åº”å¤šè¾“å‡ºé¢„æµ‹
        self.finish_output = layers.Dense(1, activation='sigmoid', name='finish')
        self.like_output   = layers.Dense(1, activation='sigmoid', name='like')

        # Attention æ¨¡å—
        self.attention = Attention(num_heads=2, key_dim=5)

        # å½’ä¸€åŒ–æ¨¡å‹
        self.norm = tf.keras.layers.LayerNormalization()

    def call(self, inputs, training=False):
        sparse_inputs = inputs[0]   # shape: (batch_size, num_sparse)
        dense_inputs  = inputs[1]   # shape: (batch_size, num_dense)
        seq_inputs    = inputs[2:] if self.seq_feats else []  # list of tensors, each shape each shape each shape: (batch_size, max_seq_len)
        # Dense è¾“å…¥:
        # [[0.211972   0.3256514 ]
        #  [0.58325326 0.5058359 ]]
        # Sparse è¾“å…¥:
        # [[5 4 1]
        #  [3 2 3]]
        # seq_inputs è¾“å…¥:
        # [array([[2, 3, 4],
        #         [3, 5, 0],
        #         [2, 4, 6]]), array([[2, 3],
        #                             [3, 4],
        #                             [2, 5]])]
        
        
        # 1ï¸âƒ£ ç¨€ç–ç‰¹å¾ embedding
        sparse_embeds = tf.stack([emb(sparse_inputs[:, i])   for i, emb in enumerate(self.sparse_embs)],axis=1)  # shape: (batch_size, num_sparse_fields, emb_dim)
        # print(sparse_embeds) shape: (batch_size=2, field_num=3, embedding_dim=5)
        # Tensor("stack:0", shape=(2, 3, 5), dtype=float32)
        # tf.Tensor(
        # [[[-0.0292243   0.03134212 -0.00664638  0.0308771   0.03662998]
        #   [-0.02252715  0.00618609 -0.0408314   0.0155008   0.00702292]
        #   [ 0.02527222  0.02442351  0.01027572  0.04815536  0.01610643]]

        #  [[ 0.01005882 -0.03315624 -0.0195043   0.01774564  0.03821408]
        #   [-0.03824542  0.00229248  0.00047214  0.0488669  -0.04776417]
        #   [-0.01696395 -0.00136379  0.04921383  0.04019973 -0.00026955]]], shape=(2, 3, 5), dtype=float32)

        sparse_flatten_embeddings = tf.reshape(sparse_embeds, shape=(-1, len(self.sparse_feats) * self.emb_size)) # shape: (batch_size, num_sparse_fields * emb_dim)
        # print(flatten_embeddings)
        # Tensor("Reshape:0", shape=(2, 15), dtype=float32)
        # tf.Tensor(
        # [[-0.03405142  0.01116457 -0.00488006  0.03367222 -0.04788997  0.0262876 0.04647902 -0.01162871  0.03328068  0.04433748  0.02085209  0.01660527
        #   -0.02046416  0.00683039  0.04853446]
        #  [-0.03806484  0.0479795   0.0132894  -0.03121579 -0.0166074   0.00733398
        #    0.00708617 -0.00899755  0.02732437 -0.00605234 -0.02896208  0.02931662
        #    0.0044607   0.03854013  0.04758653]], shape=(2, 15), dtype=float32)
        
        
        # ---------- åºåˆ—ç‰¹å¾éƒ¨åˆ† ----------
        if self.seq_feats:
            seq_embeds = []
            for i, (seq_input, seq_layer) in enumerate(zip(seq_inputs, self.seq_embeds)):
                seq_emb = seq_layer(seq_input)  # (batch_size, seq_len, emb_dim)
                pooled = tf.reduce_mean(seq_emb, axis=1, keepdims=True)  # (batch_size, 1, emb_dim)  ä»è¿™å¯ä»¥çœ‹å‡ºè¾¹é•¿åºåˆ—çš„å­—æ®µæ•°æ®æœ€ç»ˆæ•´ä½“ä¹Ÿæ˜¯å½“æˆä¸€ä¸ªå­—æ®µå¤„ç†
                seq_embeds.append(pooled)
            # æ‹¼æ¥æ‰€æœ‰åµŒå…¥ç‰¹å¾
            seq_embeds_concat = tf.concat(seq_embeds, axis=1)  # (batch_size, num_seq_fields, emb_dim)
            seq_flatten_embeddings = tf.reshape(seq_embeds_concat, shape=(-1, seq_embeds_concat.shape[1] * self.emb_size)) # shape: (batch_size, num_seq_fields * emb_dim)

            # sparseã€seqã€denseè¿›è¡Œæ‹¼æ¥
            mmoe_input = tf.concat([dense_inputs,sparse_flatten_embeddings, seq_flatten_embeddings], axis=1)    # shape=(2, X) + shape=(2, Y) + shape=(2, Z)  => shape=(2, 17))
        else:
            # sparseã€denseè¿›è¡Œæ‹¼æ¥
            mmoe_input = tf.concat([dense_inputs,sparse_flatten_embeddings], axis=1)



        # 2ï¸âƒ£ å¤šä¸“å®¶è¾“å‡º
        expert_outputs = tf.stack([expert(mmoe_input, training=training) for expert in self.experts],axis=1)  # shape: shape=(3, 4, 64) (batch_size, num_experts, expert_output_dim)

        # å¤šä¸“å®¶è¾“å‡º å¢åŠ  Transform_Attentionç‰¹å¾æå–éƒ¨åˆ†-åˆ›æ–°éƒ¨åˆ†
        # è‡ªæ³¨æ„åŠ›äº¤äº’ï¼ˆSelf-Attentionï¼‰
        residual = expert_outputs  # æ®‹å·®è¿æ¥
        expert_outputs = self.attention(expert_outputs)  # Q=K=V
        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        expert_outputs = self.norm(expert_outputs + residual)   #  åŠ  LayerNorm / æ®‹å·®è¿æ¥ æ˜¯ä¼˜åŒ–é¡¹ï¼ˆä¸å¼ºåˆ¶ï¼‰
        """
        å½’ä¸€åŒ–çš„å…·ä½“è¿‡ç¨‹:
        å¯¹å½¢çŠ¶ä¸º (2, 3, 5) çš„å¼ é‡ï¼ŒLayerNormalization çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š
        å¯¹ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆbatchï¼‰é‡Œçš„æ¯ä¸ªæ ·æœ¬ï¼ˆå…±2ä¸ªï¼‰
        å¯¹ç¬¬äºŒä¸ªç»´åº¦ï¼ˆåºåˆ—é•¿åº¦ï¼‰é‡Œçš„æ¯ä¸ªæ—¶é—´æ­¥ï¼ˆå…±3ä¸ªï¼‰
        å¯¹ç¬¬ä¸‰ä¸ªç»´åº¦ï¼ˆç‰¹å¾ç»´åº¦ï¼‰ä¸­çš„5ä¸ªå…ƒç´ è¿›è¡Œå‡å€¼å’Œæ–¹å·®è®¡ç®—ï¼Œå½’ä¸€åŒ–
        æ¢å¥è¯è¯´ï¼ŒLayerNormalization æ˜¯å¯¹å½¢çŠ¶ (5,) çš„å‘é‡åšæ ‡å‡†åŒ–ï¼Œæ¯ä¸ª (2,3) ä½ç½®éƒ½ä¼šç‹¬ç«‹è®¡ç®—å’Œå½’ä¸€åŒ–ã€‚

        ğŸ¯ ç»“è®ºï¼š
        å¯¹äºå½¢çŠ¶ä¸º (2, 3, 5) çš„è¾“å…¥ï¼š
            LayerNormalization æ˜¯å¯¹æ¯ä¸ªåºåˆ—ä¸­çš„ 5 ä¸ªç‰¹å¾å•ç‹¬åšæ ‡å‡†åŒ–
            âœ… ä¸æ˜¯ æŠŠä¸€ä¸ªæ ·æœ¬çš„ 3 Ã— 5 = 15 ä¸ªæ•°æ‹‰æˆä¸€ç»´åç»Ÿä¸€å½’ä¸€åŒ–
            æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯æ²¿ç€æœ€åä¸€ä¸ªç»´åº¦ï¼ˆfeature_dimï¼‰åšå½’ä¸€åŒ–
        
        ğŸ” ç»†èŠ‚è§£é‡Šï¼š
        ä»¥ shape = (batch_size=2, seq_len=3, feature_dim=5) ä¸ºä¾‹ï¼ŒLayerNormalization çš„å¤„ç†æ–¹å¼æ˜¯ï¼š
        for i in range(batch_size):      # å¯¹æ¯ä¸ªæ ·æœ¬
            for j in range(seq_len):     # å¯¹æ¯ä¸ªæ—¶é—´æ­¥/åºåˆ—ä½ç½®
                x = inputs[i, j, :]      # ä¸€ä¸ªé•¿åº¦ä¸º5çš„å‘é‡
                norm_x = (x - mean(x)) / sqrt(var(x) + eps)
                # å†ä¹˜ gammaï¼ŒåŠ  betaï¼ˆå¯è®­ç»ƒå‚æ•°ï¼‰
        æ‰€ä»¥ï¼Œæ¯ä¸ª (i, j) çš„ä½ç½®éƒ½ç‹¬ç«‹æ‰§è¡Œæ ‡å‡†åŒ–æ“ä½œã€‚
        """

        # 3ï¸âƒ£ æ¯ä¸ªä»»åŠ¡çš„ gate
        gate_finish_weight = tf.expand_dims(self.gate_finish(mmoe_input), axis=-1)     # (batch_size, num_experts, 1)
        gate_like_weight   = tf.expand_dims(self.gate_like(mmoe_input), axis=-1)       # (batch_size, num_experts, 1)

        # 4ï¸âƒ£ Gate åŠ æƒæ±‚å’Œï¼ˆèåˆå¤šä¸ª expertï¼‰
        task_finish_input = tf.reduce_sum(gate_finish_weight * expert_outputs,axis=1)  # (batch_size, num_experts, expert_output_dim) ==> (batch_size, expert_output_dim)
        task_like_input = tf.reduce_sum(gate_like_weight * expert_outputs,axis=1)      # (batch_size, num_experts, expert_output_dim) ==> (batch_size, expert_output_dim)
        """
        expert_outputs.shape = (2, 3, 4)
        [
            [  # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼š3ä¸ª expert è¾“å‡ºå‘é‡
                [1.0, 2.0, 3.0, 4.0],  # expert 1
                [5.0, 6.0, 7.0, 8.0],  # expert 2
                [9.0,10.0,11.0,12.0],  # expert 3
            ],
            [  # ç¬¬äºŒä¸ªæ ·æœ¬
                [0.5, 1.0, 1.5, 2.0],
                [2.0, 2.5, 3.0, 3.5],
                [4.0, 4.5, 5.0, 5.5],
            ]
        ]
        
        è¿™ä¸ªæ“ä½œè¡¨ç¤ºï¼šå¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæ²¿ç€ç¬¬ 1 ç»´ï¼ˆä¹Ÿå°±æ˜¯ä¸“å®¶æ•°é‡ï¼‰å¯¹æ¯ä¸ªä½ç½®çš„å€¼æ±‚å’Œ   
        ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼šå¯¹ä¸‰ä¸ª expert çš„å‘é‡æŒ‰åˆ—æ±‚å’Œï¼š[1.0 + 5.0 + 9.0, 2.0 + 6.0 + 10.0, 3.0 + 7.0 + 11.0, 4.0 + 8.0 + 12.0] = [15.0, 18.0, 21.0, 24.0]        
        ç¬¬äºŒä¸ªæ ·æœ¬ï¼š                            [0.5 + 2.0 + 4.0, 1.0 + 2.5 + 4.5, 1.5 + 3.0 + 5.0, 2.0 + 3.5 + 5.5]    = [6.5, 8.0, 9.5, 11.0]        

        è¾“å‡ºæ˜¯ shape (2, 4) çš„å¼ é‡ï¼š
        [
            [15.0, 18.0, 21.0, 24.0],
            [ 6.5,  8.0,  9.5, 11.0]
        ]
        reduce_sum(axis=1) å°±æ˜¯ æŠŠæ¯ä¸ªæ ·æœ¬çš„å¤šä¸ª expert è¾“å‡ºå‘é‡é€å…ƒç´ ç›¸åŠ ï¼Œæœ€ç»ˆæ¯ä¸ªæ ·æœ¬åªç•™ä¸‹ä¸€ä¸ªé•¿åº¦ä¸º expert_output_dim çš„èåˆå‘é‡ã€‚
        """


        # 5ï¸âƒ£ è¾“å‡ºé¢„æµ‹
        finish_logit = task_finish_input
        like_logit = task_like_input

        return {
            'finish': self.finish_output(finish_logit),
            'like': self.like_output(like_logit)
        }



if __name__ == '__main__':
    use_sequence = True

    if not use_sequence:
        # 1. ä¸ä½¿ç”¨åºåˆ—ç‰¹å¾
        dense_feats = ['I1', 'I2']
        sparse_feats = ['C1', 'C2', 'C3']
        feat_columns = [
            [{'feat': 'I1'}, {'feat': 'I2'}],
            [{'feat': 'C1', 'feat_num': 10}, {'feat': 'C2', 'feat_num': 8}, {'feat': 'C3', 'feat_num': 6}]
        ]

        model = MMOE_NLP_Attention(feat_columns=feat_columns, emb_size=5)
        sparse_input = np.array([[1, 2, 3], [4, 5, 5], [1, 2, 3]])
        dense_input = np.random.random((3, len(dense_feats)))
        output = model((sparse_input, dense_input), training=False)

        print("Dense è¾“å…¥:")
        print(dense_input)
        print("Sparse è¾“å…¥:")
        print(sparse_input)
        print("\næ¨¡å‹è¾“å‡º:")
        print(output)
        model.summary()
    else:
    # 2. ä½¿ç”¨åºåˆ—ç‰¹å¾
        dense_feats = ['I1', 'I2']
        sparse_feats = ['C1', 'C2', 'C3']
        sequence_feats = ['S1', 'S2']
        feat_columns = [
            [{'feat': 'I1'}, {'feat': 'I2'}],
            [{'feat': 'C1', 'feat_num': 10}, {'feat': 'C2', 'feat_num': 8}, {'feat': 'C3', 'feat_num': 6}],
            [{'feat': 'S1', 'feat_num': 10}, {'feat': 'S2', 'feat_num': 20}]
        ]

        model = MMOE_NLP_Attention(feat_columns=feat_columns, emb_size=5)
        sparse_input = np.array([[1, 2, 3], [4, 5, 5], [1, 2, 3]])
        dense_input = np.random.random((3, len(dense_feats)))

        S1 = ['movie1|movie2|movie3', 'movie2|movie5', 'movie1|movie3|movie4']
        S2 = ['movie6|movie7', 'movie7|movie8', 'movie6|movie9']
        sequence_inputs = {}
        tokenizers = {}
        for feat, texts in zip(sequence_feats, [S1, S2]):
            tokenizer = Tokenizer(oov_token='OOV')
            tokenizer.fit_on_texts(texts)
            padded = pad_sequences(tokenizer.texts_to_sequences(texts), padding='post')
            sequence_inputs[feat] = padded
            tokenizers[feat] = tokenizer

        seq_input_list = [sequence_inputs[feat] for feat in sequence_feats]
        output = model((sparse_input, dense_input, *seq_input_list), training=False)

        print("Dense è¾“å…¥:")
        print(dense_input)
        print("Sparse è¾“å…¥:")
        print(sparse_input)
        for feat in sequence_feats:
            print(f"{feat} è¾“å…¥:")
            print(sequence_inputs[feat])
        print("\næ¨¡å‹è¾“å‡º:")
        print(output)

        model.summary()

# Dense è¾“å…¥:
# [[0.113212   0.63990683]
#  [0.83283576 0.00455242]
#  [0.40675872 0.30803841]]
# Sparse è¾“å…¥:
# [[1 2 3]
#  [4 5 5]
#  [1 2 3]]
# S1 è¾“å…¥:
# [[2 3 4]
#  [3 5 0]
#  [2 4 6]]
# S2 è¾“å…¥:
# [[2 3]
#  [3 4]
#  [2 5]]
#
# æ¨¡å‹è¾“å‡º:
# {'finish': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
# array([[0.5065163 ],
#        [0.51338285],
#        [0.50972116]], dtype=float32)>, 'like': <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
# array([[0.47611362],
#        [0.47083804],
#        [0.48120457]], dtype=float32)>}
# Model: "MMOE_NLP_Attention"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #
# =================================================================
# embedding (Embedding)        multiple                  50
# _________________________________________________________________
# embedding_1 (Embedding)      multiple                  40
# _________________________________________________________________
# embedding_2 (Embedding)      multiple                  30
# _________________________________________________________________
# embedding_3 (Embedding)      multiple                  50
# _________________________________________________________________
# embedding_4 (Embedding)      multiple                  100
# _________________________________________________________________
# sequential (Sequential)      (3, 64)                   11840
# _________________________________________________________________
# sequential_1 (Sequential)    (3, 64)                   11840
# _________________________________________________________________
# sequential_2 (Sequential)    (3, 64)                   11840
# _________________________________________________________________
# sequential_3 (Sequential)    (3, 64)                   11840
# _________________________________________________________________
# dense_8 (Dense)              multiple                  112
# _________________________________________________________________
# dense_9 (Dense)              multiple                  112
# _________________________________________________________________
# finish (Dense)               multiple                  65
# _________________________________________________________________
# like (Dense)                 multiple                  65
# =================================================================
# Total params: 47,984
# Trainable params: 47,984
# Non-trainable params: 0
# _________________________________________________________________