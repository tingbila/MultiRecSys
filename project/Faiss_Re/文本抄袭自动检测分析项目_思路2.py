# !/usr/bin/env python
# -*- coding:utf-8 -*-
# @Time  : 2018.
# @Author : å¼ æ˜é˜³
# @Email : mingyang.zhang@ushow.media

import numpy as np
import pandas as pd
import pickle
import os
import re
import jieba
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
import faiss


# ğŸ” å·¥ä½œæœºåˆ¶è¯´æ˜ï¼š
#     æ–‡æœ¬è¡¨ç¤ºï¼š
#         æ¯ç¯‡æ–‡ç« é€šè¿‡åˆ†è¯ â†’ CountVectorizer â†’ TF-IDF è½¬æ¢æˆå‘é‡ tfidfã€‚
#     ç´¢å¼•å»ºç«‹ï¼š
#         ä½¿ç”¨ faiss.IndexFlatL2 æ„å»ºç´¢å¼•ï¼Œè®¡ç®—çš„æ˜¯ L2 æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚
#     æŸ¥è¯¢æ£€ç´¢ï¼š
#         é€‰å–å¾…æ£€æµ‹æ–‡ç«  query_self = tfidf[cpindex:cpindex+1]ã€‚
#          index.search(query_self, k) è¿”å›æœ€ç›¸ä¼¼çš„ k ç¯‡æ–‡ç« çš„ç´¢å¼•ä¸è·ç¦»ã€‚


# ----------------------
# 1. ä¸­æ–‡æ–‡æœ¬åˆ†è¯å‡½æ•°å®šä¹‰
# ----------------------
def split_text(text):
    """
    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œä¸­æ–‡åˆ†è¯ï¼Œæ¸…é™¤éä¸­æ–‡å­—ç¬¦ã€‚
    :param text: åŸå§‹å­—ç¬¦ä¸²æ–‡æœ¬
    :return: åˆ†è¯åçš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œä»¥ç©ºæ ¼è¿æ¥
    """
    # æ¸…é™¤éä¸­æ–‡å­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fa5]', '', text)
    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    words = jieba.cut(text)
    return ' '.join(words)

# ----------------------
# 2. æ•°æ®è¯»å–ä¸é¢„å¤„ç†
# ----------------------
news = pd.read_csv(r'D:\software\pycharm_repository\StarMaker\MultiRecSys\data_files\sqlResult.csv', encoding='utf-8')


# åˆ é™¤ content ä¸ºç©ºçš„è¡Œ
print(news[news.content.isna()].head(5))
news = news.dropna(subset=['content'])

# ----------------------
# 3. åˆ†è¯ç»“æœç¼“å­˜ä¸åŠ è½½
# ----------------------
corpus_path = "corpus.pkl"
if not os.path.exists(corpus_path):
    corpus = list(map(split_text, [str(i) for i in news.content]))
    print(corpus[0])
    print(len(corpus))
    print(corpus[1])
    with open(corpus_path, 'wb') as file:
        pickle.dump(corpus, file)
else:
    with open(corpus_path, 'rb') as file:
        corpus = pickle.load(file)

# ----------------------
# 4. TF-IDF ç‰¹å¾æå–
# ----------------------
tfidf_path = "tfidf.pkl"
if not os.path.exists(tfidf_path):
    countvectorizer = CountVectorizer(min_df=0.015)  # å»é™¤ä½é¢‘è¯
    tfidftransformer = TfidfTransformer()
    countvector = countvectorizer.fit_transform(corpus)  # è¯é¢‘ç»Ÿè®¡çŸ©é˜µ
    tfidf = tfidftransformer.fit_transform(countvector)  # TF-IDF çŸ©é˜µ
    print(countvector.shape)
    print(tfidf.shape)
    with open(tfidf_path, 'wb') as file:
        pickle.dump(tfidf, file)
else:
    with open(tfidf_path, 'rb') as file:
        tfidf = pickle.load(file)

# è½¬æ¢ä¸º NumPy æ ¼å¼å¹¶é™ä½ç²¾åº¦ï¼ˆèŠ‚çœå†…å­˜ï¼‰
tfidf = tfidf.toarray().astype(np.float32)
d = tfidf.shape[1]  # å‘é‡ç»´åº¦
print(d)
print(tfidf.shape)
print(type(tfidf))
print(type(tfidf[1][1]))

# ----------------------
# 5. FAISS å‘é‡æœç´¢æ„å»ºç´¢å¼•
# ----------------------
index = faiss.IndexFlatL2(d)  # ä½¿ç”¨æ¬§æ°è·ç¦»æ„å»ºç´¢å¼•ï¼ˆæ— éœ€è®­ç»ƒï¼‰
print(index.is_trained)  # éªŒè¯æ˜¯å¦è®­ç»ƒï¼ˆå¯¹äº IndexFlatL2 æ€»ä¸º Trueï¼‰
index.add(tfidf)  # æ·»åŠ å…¨éƒ¨æ–‡æœ¬å‘é‡åˆ°ç´¢å¼•ä¸­
print(index.ntotal)  # ç´¢å¼•ä¸­å‘é‡ä¸ªæ•°

# ----------------------
# 6. æŸ¥è¯¢ç›¸ä¼¼æ–‡æœ¬ï¼ˆæ¨¡æ‹ŸæŠ„è¢­æ£€æµ‹ï¼‰
# ----------------------
k = 10  # è¿”å›æœ€ç›¸ä¼¼çš„å‰kæ¡è®°å½•
cpindex = 3352  # æŒ‡å®šå¾…æ£€æµ‹æ–‡æœ¬çš„ç´¢å¼•
query_self = tfidf[cpindex:cpindex + 1]  # å–å‡ºæŸ¥è¯¢å‘é‡

# æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
dis, ind = index.search(query_self, k)

print(dis.shape)
print(ind.shape)
print(dis)  # ç›¸ä¼¼åº¦ï¼ˆæ¬§æ°è·ç¦»ï¼‰
print(ind)  # ç›¸ä¼¼æ–‡æœ¬çš„ç´¢å¼•

# è¾“å‡ºç›¸ä¼¼æ–‡æœ¬å¯¹å†…å®¹
print('æ€€ç–‘æŠ„è¢­:\n', news.iloc[cpindex].content)
similar2 = ind[0][1]  # ç¬¬äºŒä¸ªæœ€ç›¸ä¼¼çš„æ˜¯ç›¸ä¼¼æ–‡æœ¬ï¼ˆç¬¬ä¸€ä¸ªæ˜¯è‡ªå·±ï¼‰
print(similar2)
print('ç›¸ä¼¼åŸæ–‡:\n', news.iloc[similar2].content)
